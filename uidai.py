# -*- coding: utf-8 -*-
"""UIDAI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jdHKj37kVkBHeJHyzB_6HAf8MJPbE6QC
"""

# Step 1: Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Step 2: Import libraries
import pandas as pd
import glob
import os

# Step 3: Set your exact folder path
base_path = '/content/drive/MyDrive/UIDAI_Hackathon/'

# Step 4: Merge ENROLMENT data
print("Loading Enrolment Data...")
enrolment_files = glob.glob(base_path + 'api_data_aadhar_enrolment/*.csv')
enrolment_files.sort()  # Sort to maintain order

enrolment_dfs = []
for file in enrolment_files:
    print(f"Reading: {os.path.basename(file)}")
    df = pd.read_csv(file)
    enrolment_dfs.append(df)

enrolment_data = pd.concat(enrolment_dfs, ignore_index=True)
print(f"\n‚úì Enrolment Data Merged: {enrolment_data.shape[0]:,} rows, {enrolment_data.shape[1]} columns")
print(enrolment_data.head())

# Step 5: Merge DEMOGRAPHIC data
print("\n" + "="*50)
print("Loading Demographic Data...")
demographic_files = glob.glob(base_path + 'api_data_aadhar_demographic/*.csv')
demographic_files.sort()

demographic_dfs = []
for file in demographic_files:
    print(f"Reading: {os.path.basename(file)}")
    df = pd.read_csv(file)
    demographic_dfs.append(df)

demographic_data = pd.concat(demographic_dfs, ignore_index=True)
print(f"\n‚úì Demographic Data Merged: {demographic_data.shape[0]:,} rows, {demographic_data.shape[1]} columns")
print(demographic_data.head())

# Step 6: Merge BIOMETRIC data
print("\n" + "="*50)
print("Loading Biometric Data...")
biometric_files = glob.glob(base_path + 'api_data_aadhar_biometric/*.csv')
biometric_files.sort()

biometric_dfs = []
for file in biometric_files:
    print(f"Reading: {os.path.basename(file)}")
    df = pd.read_csv(file)
    biometric_dfs.append(df)

biometric_data = pd.concat(biometric_dfs, ignore_index=True)
print(f"\n‚úì Biometric Data Merged: {biometric_data.shape[0]:,} rows, {biometric_data.shape[1]} columns")
print(biometric_data.head())

# Step 7: Summary
print("\n" + "="*50)
print("MERGE COMPLETE!")
print(f"Enrolment: {enrolment_data.shape[0]:,} rows")
print(f"Demographic: {demographic_data.shape[0]:,} rows")
print(f"Biometric: {biometric_data.shape[0]:,} rows")

# Check column names
print("Enrolment Columns:", enrolment_data.columns.tolist())
print("\nDemographic Columns:", demographic_data.columns.tolist())
print("\nBiometric Columns:", biometric_data.columns.tolist())

# Check data types
print("\nEnrolment Data Types:")
print(enrolment_data.dtypes)

# ============================================
# COMPREHENSIVE DATA ANALYSIS
# ============================================

import numpy as np

print("="*70)
print("DATASET OVERVIEW")
print("="*70)

datasets = {
    'Enrolment': enrolment_data,
    'Demographic': demographic_data,
    'Biometric': biometric_data
}

for name, df in datasets.items():
    print(f"\n{name} Dataset:")
    print(f"  Rows: {df.shape[0]:,}")
    print(f"  Columns: {df.shape[1]}")
    print(f"  Memory Usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")

# ============================================
# MISSING VALUES ANALYSIS
# ============================================

print("\n" + "="*70)
print("MISSING VALUES CHECK")
print("="*70)

for name, df in datasets.items():
    print(f"\n{name} Dataset:")
    missing = df.isnull().sum()
    missing_pct = (df.isnull().sum() / len(df)) * 100
    missing_df = pd.DataFrame({
        'Column': missing.index,
        'Missing_Count': missing.values,
        'Missing_Percentage': missing_pct.values
    })
    print(missing_df[missing_df['Missing_Count'] > 0])
    if missing.sum() == 0:
        print("  ‚úì No missing values!")

# ============================================
# DATE RANGE ANALYSIS
# ============================================

print("\n" + "="*70)
print("DATE RANGE ANALYSIS")
print("="*70)

for name, df in datasets.items():
    print(f"\n{name} Dataset:")
    print(f"  Earliest Date: {df['date'].min()}")
    print(f"  Latest Date: {df['date'].max()}")
    print(f"  Unique Dates: {df['date'].nunique()}")
    print(f"  Date Range: {df['date'].nunique()} days")

# ============================================
# GEOGRAPHICAL COVERAGE
# ============================================

print("\n" + "="*70)
print("GEOGRAPHICAL COVERAGE")
print("="*70)

for name, df in datasets.items():
    print(f"\n{name} Dataset:")
    print(f"  Unique States: {df['state'].nunique()}")
    print(f"  Unique Districts: {df['district'].nunique()}")
    print(f"  Unique Pincodes: {df['pincode'].nunique()}")

# ============================================
# DESCRIPTIVE STATISTICS - ENROLMENT
# ============================================

print("\n" + "="*70)
print("DESCRIPTIVE STATISTICS - ENROLMENT")
print("="*70)
print(enrolment_data[['age_0_5', 'age_5_17', 'age_18_greater']].describe())

print("\nTotal Enrolments by Age Group:")
print(f"  Age 0-5: {enrolment_data['age_0_5'].sum():,}")
print(f"  Age 5-17: {enrolment_data['age_5_17'].sum():,}")
print(f"  Age 18+: {enrolment_data['age_18_greater'].sum():,}")
print(f"  Total: {enrolment_data[['age_0_5', 'age_5_17', 'age_18_greater']].sum().sum():,}")

# ============================================
# DESCRIPTIVE STATISTICS - DEMOGRAPHIC UPDATES
# ============================================

print("\n" + "="*70)
print("DESCRIPTIVE STATISTICS - DEMOGRAPHIC UPDATES")
print("="*70)
print(demographic_data[['demo_age_5_17', 'demo_age_17_']].describe())

print("\nTotal Demographic Updates by Age Group:")
print(f"  Age 5-17: {demographic_data['demo_age_5_17'].sum():,}")
print(f"  Age 17+: {demographic_data['demo_age_17_'].sum():,}")
print(f"  Total: {demographic_data[['demo_age_5_17', 'demo_age_17_']].sum().sum():,}")

# ============================================
# DESCRIPTIVE STATISTICS - BIOMETRIC UPDATES
# ============================================

print("\n" + "="*70)
print("DESCRIPTIVE STATISTICS - BIOMETRIC UPDATES")
print("="*70)
print(biometric_data[['bio_age_5_17', 'bio_age_17_']].describe())

print("\nTotal Biometric Updates by Age Group:")
print(f"  Age 5-17: {biometric_data['bio_age_5_17'].sum():,}")
print(f"  Age 17+: {biometric_data['bio_age_17_'].sum():,}")
print(f"  Total: {biometric_data[['bio_age_5_17', 'bio_age_17_']].sum().sum():,}")

# ============================================
# TOP STATES ANALYSIS
# ============================================

print("\n" + "="*70)
print("TOP 10 STATES BY ACTIVITY")
print("="*70)

print("\nEnrolment - Top 10 States:")
print(enrolment_data['state'].value_counts().head(10))

print("\nDemographic Updates - Top 10 States:")
print(demographic_data['state'].value_counts().head(10))

print("\nBiometric Updates - Top 10 States:")
print(biometric_data['state'].value_counts().head(10))

# ============================================
# DATA QUALITY CHECKS
# ============================================

print("\n" + "="*70)
print("DATA QUALITY CHECKS")
print("="*70)

# Check for zeros
print("\nEnrolment - Rows with all zeros in age columns:")
zero_enrol = enrolment_data[(enrolment_data['age_0_5'] == 0) &
                             (enrolment_data['age_5_17'] == 0) &
                             (enrolment_data['age_18_greater'] == 0)]
print(f"  Count: {len(zero_enrol)} ({len(zero_enrol)/len(enrolment_data)*100:.2f}%)")

print("\nDemographic - Rows with all zeros:")
zero_demo = demographic_data[(demographic_data['demo_age_5_17'] == 0) &
                              (demographic_data['demo_age_17_'] == 0)]
print(f"  Count: {len(zero_demo)} ({len(zero_demo)/len(demographic_data)*100:.2f}%)")

print("\nBiometric - Rows with all zeros:")
zero_bio = biometric_data[(biometric_data['bio_age_5_17'] == 0) &
                           (biometric_data['bio_age_17_'] == 0)]
print(f"  Count: {len(zero_bio)} ({len(zero_bio)/len(biometric_data)*100:.2f}%)")

# Check duplicate rows
print("\n" + "="*70)
print("DUPLICATE ROWS CHECK")
print("="*70)

for name, df in datasets.items():
    duplicates = df.duplicated().sum()
    print(f"{name}: {duplicates:,} duplicate rows ({duplicates/len(df)*100:.2f}%)")

print("\n" + "="*70)
print("ANALYSIS COMPLETE!")
print("="*70)

# STEP 1: DATA CLEANING (Run this first)
print("Starting Data Cleaning...")

# Remove duplicates
enrolment_clean = enrolment_data.drop_duplicates()
demographic_clean = demographic_data.drop_duplicates()
biometric_clean = biometric_data.drop_duplicates()

# Remove zero rows
demographic_clean = demographic_clean[
    (demographic_clean['demo_age_5_17'] > 0) |
    (demographic_clean['demo_age_17_'] > 0)
]
biometric_clean = biometric_clean[
    (biometric_clean['bio_age_5_17'] > 0) |
    (biometric_clean['bio_age_17_'] > 0)
]

# Convert date to datetime
enrolment_clean['date'] = pd.to_datetime(enrolment_clean['date'], format='%d-%m-%Y')
demographic_clean['date'] = pd.to_datetime(demographic_clean['date'], format='%d-%m-%Y')
biometric_clean['date'] = pd.to_datetime(biometric_clean['date'], format='%d-%m-%Y')

# Add derived features
enrolment_clean['month'] = enrolment_clean['date'].dt.month
enrolment_clean['month_name'] = enrolment_clean['date'].dt.month_name()
enrolment_clean['total_enrolments'] = (enrolment_clean['age_0_5'] +
                                        enrolment_clean['age_5_17'] +
                                        enrolment_clean['age_18_greater'])

demographic_clean['month'] = demographic_clean['date'].dt.month
demographic_clean['month_name'] = demographic_clean['date'].dt.month_name()
demographic_clean['total_updates'] = (demographic_clean['demo_age_5_17'] +
                                       demographic_clean['demo_age_17_'])

biometric_clean['month'] = biometric_clean['date'].dt.month
biometric_clean['month_name'] = biometric_clean['date'].dt.month_name()
biometric_clean['total_updates'] = (biometric_clean['bio_age_5_17'] +
                                     biometric_clean['bio_age_17_'])

# Add region classification (first digit of pincode)
enrolment_clean['region'] = (enrolment_clean['pincode'] // 100000).astype(str)
demographic_clean['region'] = (demographic_clean['pincode'] // 100000).astype(str)
biometric_clean['region'] = (biometric_clean['pincode'] // 100000).astype(str)

print("‚úì Data Cleaning Complete!")
print(f"Enrolment: {len(enrolment_clean):,} rows (removed {len(enrolment_data) - len(enrolment_clean):,} duplicates)")
print(f"Demographic: {len(demographic_clean):,} rows (removed {len(demographic_data) - len(demographic_clean):,} duplicates/zeros)")
print(f"Biometric: {len(biometric_clean):,} rows (removed {len(biometric_data) - len(biometric_clean):,} duplicates/zeros)")

"""# üîç ANALYSIS 1: TEMPORAL TRENDS & SEASONALITY"""

import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

# Set style for better-looking plots
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (14, 6)
plt.rcParams['font.size'] = 10

print("="*70)
print("ANALYSIS 1: TEMPORAL TRENDS & SEASONALITY")
print("="*70)

# Monthly trends for all datasets
enrol_monthly = enrolment_clean.groupby('month_name').agg({
    'age_0_5': 'sum',
    'age_5_17': 'sum',
    'age_18_greater': 'sum',
    'total_enrolments': 'sum'
}).reset_index()

demo_monthly = demographic_clean.groupby('month_name').agg({
    'demo_age_5_17': 'sum',
    'demo_age_17_': 'sum',
    'total_updates': 'sum'
}).reset_index()

bio_monthly = biometric_clean.groupby('month_name').agg({
    'bio_age_5_17': 'sum',
    'bio_age_17_': 'sum',
    'total_updates': 'sum'
}).reset_index()

# Sort by month order
month_order = ['March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']
enrol_monthly['month_name'] = pd.Categorical(enrol_monthly['month_name'], categories=month_order, ordered=True)
demo_monthly['month_name'] = pd.Categorical(demo_monthly['month_name'], categories=month_order, ordered=True)
bio_monthly['month_name'] = pd.Categorical(bio_monthly['month_name'], categories=month_order, ordered=True)

enrol_monthly = enrol_monthly.sort_values('month_name')
demo_monthly = demo_monthly.sort_values('month_name')
bio_monthly = bio_monthly.sort_values('month_name')

print("\nENROLMENT - Monthly Trends:")
print(enrol_monthly)

print("\nDEMOGRAPHIC UPDATES - Monthly Trends:")
print(demo_monthly)

print("\nBIOMETRIC UPDATES - Monthly Trends:")
print(bio_monthly)

# GRAPH 1: Overall Monthly Trends Comparison
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

axes[0].plot(enrol_monthly['month_name'], enrol_monthly['total_enrolments'],
             marker='o', linewidth=2, markersize=8, color='#2E86AB')
axes[0].set_title('Enrolment Trends Over Months', fontsize=14, fontweight='bold')
axes[0].set_xlabel('Month', fontsize=11)
axes[0].set_ylabel('Total Enrolments', fontsize=11)
axes[0].tick_params(axis='x', rotation=45)
axes[0].grid(True, alpha=0.3)

axes[1].plot(demo_monthly['month_name'], demo_monthly['total_updates'],
             marker='s', linewidth=2, markersize=8, color='#A23B72')
axes[1].set_title('Demographic Updates Trends Over Months', fontsize=14, fontweight='bold')
axes[1].set_xlabel('Month', fontsize=11)
axes[1].set_ylabel('Total Updates', fontsize=11)
axes[1].tick_params(axis='x', rotation=45)
axes[1].grid(True, alpha=0.3)

axes[2].plot(bio_monthly['month_name'], bio_monthly['total_updates'],
             marker='^', linewidth=2, markersize=8, color='#F18F01')
axes[2].set_title('Biometric Updates Trends Over Months', fontsize=14, fontweight='bold')
axes[2].set_xlabel('Month', fontsize=11)
axes[2].set_ylabel('Total Updates', fontsize=11)
axes[2].tick_params(axis='x', rotation=45)
axes[2].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# Calculate growth rates
enrol_monthly['growth_rate'] = enrol_monthly['total_enrolments'].pct_change() * 100
demo_monthly['growth_rate'] = demo_monthly['total_updates'].pct_change() * 100
bio_monthly['growth_rate'] = bio_monthly['total_updates'].pct_change() * 100

print("\n" + "="*50)
print("GROWTH RATE ANALYSIS (Month-over-Month %)")
print("="*50)
print("\nEnrolment Growth:")
print(enrol_monthly[['month_name', 'total_enrolments', 'growth_rate']])

print("\nDemographic Updates Growth:")
print(demo_monthly[['month_name', 'total_updates', 'growth_rate']])

print("\nBiometric Updates Growth:")
print(bio_monthly[['month_name', 'total_updates', 'growth_rate']])

# GRAPH 2: Growth Rate Comparison
fig, ax = plt.subplots(figsize=(14, 6))

x = range(len(enrol_monthly['month_name']))
width = 0.25

ax.bar([i - width for i in x], enrol_monthly['growth_rate'].fillna(0),
       width, label='Enrolment', color='#2E86AB', alpha=0.8)
ax.bar(x, demo_monthly['growth_rate'].fillna(0),
       width, label='Demographic', color='#A23B72', alpha=0.8)
ax.bar([i + width for i in x], bio_monthly['growth_rate'].fillna(0),
       width, label='Biometric', color='#F18F01', alpha=0.8)

ax.set_xlabel('Month', fontsize=12)
ax.set_ylabel('Growth Rate (%)', fontsize=12)
ax.set_title('Month-over-Month Growth Rate Comparison', fontsize=14, fontweight='bold')
ax.set_xticks(x)
ax.set_xticklabels(enrol_monthly['month_name'], rotation=45, ha='right')
ax.legend(fontsize=11)
ax.axhline(y=0, color='black', linestyle='-', linewidth=0.5)
ax.grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.show()

# GRAPH 3: Age Group Trends in Enrolment Over Time
fig, ax = plt.subplots(figsize=(14, 6))

ax.plot(enrol_monthly['month_name'], enrol_monthly['age_0_5'],
        marker='o', linewidth=2.5, label='Age 0-5', color='#06A77D')
ax.plot(enrol_monthly['month_name'], enrol_monthly['age_5_17'],
        marker='s', linewidth=2.5, label='Age 5-17', color='#D62828')
ax.plot(enrol_monthly['month_name'], enrol_monthly['age_18_greater'],
        marker='^', linewidth=2.5, label='Age 18+', color='#F77F00')

ax.set_xlabel('Month', fontsize=12)
ax.set_ylabel('Number of Enrolments', fontsize=12)
ax.set_title('Enrolment Trends by Age Group Over Months', fontsize=14, fontweight='bold')
ax.legend(fontsize=11, loc='best')
ax.tick_params(axis='x', rotation=45)
ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# Day of week analysis
enrolment_clean['day_of_week'] = enrolment_clean['date'].dt.day_name()
demographic_clean['day_of_week'] = demographic_clean['date'].dt.day_name()
biometric_clean['day_of_week'] = biometric_clean['date'].dt.day_name()

print("\n" + "="*50)
print("DAY OF WEEK PATTERNS")
print("="*50)

dow_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']

enrol_dow = enrolment_clean.groupby('day_of_week')['total_enrolments'].sum().reindex(dow_order)
demo_dow = demographic_clean.groupby('day_of_week')['total_updates'].sum().reindex(dow_order)
bio_dow = biometric_clean.groupby('day_of_week')['total_updates'].sum().reindex(dow_order)

print("\nEnrolment by Day:")
print(enrol_dow)
print("\nDemographic Updates by Day:")
print(demo_dow)
print("\nBiometric Updates by Day:")
print(bio_dow)

# GRAPH 4: Day of Week Heatmap
fig, ax = plt.subplots(figsize=(12, 6))

dow_data = pd.DataFrame({
    'Enrolment': enrol_dow.values,
    'Demographic': demo_dow.values,
    'Biometric': bio_dow.values
}, index=dow_order)

# Normalize for better comparison
dow_data_normalized = dow_data.div(dow_data.sum(axis=0), axis=1) * 100

x_pos = np.arange(len(dow_order))
width = 0.25

ax.bar(x_pos - width, dow_data['Enrolment'], width, label='Enrolment', color='#2E86AB', alpha=0.8)
ax.bar(x_pos, dow_data['Demographic'], width, label='Demographic', color='#A23B72', alpha=0.8)
ax.bar(x_pos + width, dow_data['Biometric'], width, label='Biometric', color='#F18F01', alpha=0.8)

ax.set_xlabel('Day of Week', fontsize=12)
ax.set_ylabel('Total Count', fontsize=12)
ax.set_title('Activity Distribution by Day of Week', fontsize=14, fontweight='bold')
ax.set_xticks(x_pos)
ax.set_xticklabels(dow_order, rotation=45, ha='right')
ax.legend(fontsize=11)
ax.grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.show()

print("\n‚úì Analysis 1 Complete!")

"""# üó∫Ô∏è ANALYSIS 2: GEOGRAPHIC DISPARITIES & EQUITY"""

print("\n" + "="*70)
print("ANALYSIS 2: GEOGRAPHIC DISPARITIES & EQUITY")
print("="*70)

# State-level analysis
state_enrol = enrolment_clean.groupby('state')['total_enrolments'].sum().sort_values(ascending=False)
state_demo = demographic_clean.groupby('state')['total_updates'].sum().sort_values(ascending=False)
state_bio = biometric_clean.groupby('state')['total_updates'].sum().sort_values(ascending=False)

print("\nTOP 10 States - Enrolments:")
print(state_enrol.head(10))

print("\nBOTTOM 10 States - Enrolments:")
print(state_enrol.tail(10))

# GRAPH 1: Top vs Bottom States Comparison
fig, axes = plt.subplots(2, 3, figsize=(18, 10))

# Top 10 Enrolments
axes[0, 0].barh(state_enrol.head(10).index, state_enrol.head(10).values, color='#2E86AB')
axes[0, 0].set_xlabel('Total Enrolments', fontsize=11)
axes[0, 0].set_title('Top 10 States - Enrolments', fontsize=12, fontweight='bold')
axes[0, 0].invert_yaxis()
axes[0, 0].grid(True, alpha=0.3, axis='x')

# Bottom 10 Enrolments
axes[1, 0].barh(state_enrol.tail(10).index, state_enrol.tail(10).values, color='#E63946')
axes[1, 0].set_xlabel('Total Enrolments', fontsize=11)
axes[1, 0].set_title('Bottom 10 States - Enrolments', fontsize=12, fontweight='bold')
axes[1, 0].invert_yaxis()
axes[1, 0].grid(True, alpha=0.3, axis='x')

# Top 10 Demographic
axes[0, 1].barh(state_demo.head(10).index, state_demo.head(10).values, color='#A23B72')
axes[0, 1].set_xlabel('Total Updates', fontsize=11)
axes[0, 1].set_title('Top 10 States - Demographic Updates', fontsize=12, fontweight='bold')
axes[0, 1].invert_yaxis()
axes[0, 1].grid(True, alpha=0.3, axis='x')

# Bottom 10 Demographic
axes[1, 1].barh(state_demo.tail(10).index, state_demo.tail(10).values, color='#E63946')
axes[1, 1].set_xlabel('Total Updates', fontsize=11)
axes[1, 1].set_title('Bottom 10 States - Demographic Updates', fontsize=12, fontweight='bold')
axes[1, 1].invert_yaxis()
axes[1, 1].grid(True, alpha=0.3, axis='x')

# Top 10 Biometric
axes[0, 2].barh(state_bio.head(10).index, state_bio.head(10).values, color='#F18F01')
axes[0, 2].set_xlabel('Total Updates', fontsize=11)
axes[0, 2].set_title('Top 10 States - Biometric Updates', fontsize=12, fontweight='bold')
axes[0, 2].invert_yaxis()
axes[0, 2].grid(True, alpha=0.3, axis='x')

# Bottom 10 Biometric
axes[1, 2].barh(state_bio.tail(10).index, state_bio.tail(10).values, color='#E63946')
axes[1, 2].set_xlabel('Total Updates', fontsize=11)
axes[1, 2].set_title('Bottom 10 States - Biometric Updates', fontsize=12, fontweight='bold')
axes[1, 2].invert_yaxis()
axes[1, 2].grid(True, alpha=0.3, axis='x')

plt.tight_layout()
plt.show()

# Per-record average (intensity analysis)
state_enrol_avg = enrolment_clean.groupby('state')['total_enrolments'].mean().sort_values(ascending=False)
state_demo_avg = demographic_clean.groupby('state')['total_updates'].mean().sort_values(ascending=False)
state_bio_avg = biometric_clean.groupby('state')['total_updates'].mean().sort_values(ascending=False)

print("\n" + "="*50)
print("AVERAGE ACTIVITY PER RECORD (Intensity)")
print("="*50)

print("\nTOP 10 States - Average Enrolments per Record:")
print(state_enrol_avg.head(10))

print("\nTOP 10 States - Average Demographic Updates per Record:")
print(state_demo_avg.head(10))

print("\nTOP 10 States - Average Biometric Updates per Record:")
print(state_bio_avg.head(10))

# GRAPH 2: Intensity Analysis
fig, axes = plt.subplots(1, 3, figsize=(18, 6))

axes[0].barh(state_enrol_avg.head(10).index, state_enrol_avg.head(10).values, color='#06A77D')
axes[0].set_xlabel('Avg Enrolments per Record', fontsize=11)
axes[0].set_title('Top 10 States by Enrolment Intensity', fontsize=12, fontweight='bold')
axes[0].invert_yaxis()
axes[0].grid(True, alpha=0.3, axis='x')

axes[1].barh(state_demo_avg.head(10).index, state_demo_avg.head(10).values, color='#6A4C93')
axes[1].set_xlabel('Avg Updates per Record', fontsize=11)
axes[1].set_title('Top 10 States by Demographic Intensity', fontsize=12, fontweight='bold')
axes[1].invert_yaxis()
axes[1].grid(True, alpha=0.3, axis='x')

axes[2].barh(state_bio_avg.head(10).index, state_bio_avg.head(10).values, color='#FF6B35')
axes[2].set_xlabel('Avg Updates per Record', fontsize=11)
axes[2].set_title('Top 10 States by Biometric Intensity', fontsize=12, fontweight='bold')
axes[2].invert_yaxis()
axes[2].grid(True, alpha=0.3, axis='x')

plt.tight_layout()
plt.show()

# District-level disparities
district_enrol = enrolment_clean.groupby(['state', 'district'])['total_enrolments'].sum().sort_values(ascending=False)
district_demo = demographic_clean.groupby(['state', 'district'])['total_updates'].sum().sort_values(ascending=False)
district_bio = biometric_clean.groupby(['state', 'district'])['total_updates'].sum().sort_values(ascending=False)

print("\n" + "="*50)
print("TOP 15 DISTRICTS (Overall)")
print("="*50)

print("\nEnrolments:")
print(district_enrol.head(15))

print("\nDemographic Updates:")
print(district_demo.head(15))

print("\nBiometric Updates:")
print(district_bio.head(15))

# GRAPH 3: Top Districts
fig, axes = plt.subplots(1, 3, figsize=(18, 7))

# Format district names for better readability
top_districts_enrol = district_enrol.head(12)
district_labels_enrol = [f"{d[1]}, {d[0]}" for d in top_districts_enrol.index]

top_districts_demo = district_demo.head(12)
district_labels_demo = [f"{d[1]}, {d[0]}" for d in top_districts_demo.index]

top_districts_bio = district_bio.head(12)
district_labels_bio = [f"{d[1]}, {d[0]}" for d in top_districts_bio.index]

axes[0].barh(district_labels_enrol, top_districts_enrol.values, color='#2E86AB')
axes[0].set_xlabel('Total Enrolments', fontsize=11)
axes[0].set_title('Top 12 Districts - Enrolments', fontsize=12, fontweight='bold')
axes[0].invert_yaxis()
axes[0].grid(True, alpha=0.3, axis='x')

axes[1].barh(district_labels_demo, top_districts_demo.values, color='#A23B72')
axes[1].set_xlabel('Total Updates', fontsize=11)
axes[1].set_title('Top 12 Districts - Demographic Updates', fontsize=12, fontweight='bold')
axes[1].invert_yaxis()
axes[1].grid(True, alpha=0.3, axis='x')

axes[2].barh(district_labels_bio, top_districts_bio.values, color='#F18F01')
axes[2].set_xlabel('Total Updates', fontsize=11)
axes[2].set_title('Top 12 Districts - Biometric Updates', fontsize=12, fontweight='bold')
axes[2].invert_yaxis()
axes[2].grid(True, alpha=0.3, axis='x')

plt.tight_layout()
plt.show()

# Regional analysis (based on pincode first digit)
region_enrol = enrolment_clean.groupby('region')['total_enrolments'].sum().sort_values(ascending=False)
region_demo = demographic_clean.groupby('region')['total_updates'].sum().sort_values(ascending=False)
region_bio = biometric_clean.groupby('region')['total_updates'].sum().sort_values(ascending=False)

print("\n" + "="*50)
print("REGIONAL ANALYSIS (Pincode-based)")
print("="*50)
print("\nEnrolments by Region:")
print(region_enrol)
print("\nDemographic Updates by Region:")
print(region_demo)
print("\nBiometric Updates by Region:")
print(region_bio)

# GRAPH 4: Regional Comparison
fig, ax = plt.subplots(figsize=(14, 6))

regions = region_enrol.index
x_pos = np.arange(len(regions))
width = 0.25

# Normalize for comparison
enrol_norm = region_enrol.values / region_enrol.values.max() * 100
demo_norm = region_demo.reindex(regions).fillna(0).values / region_demo.values.max() * 100
bio_norm = region_bio.reindex(regions).fillna(0).values / region_bio.values.max() * 100

ax.bar(x_pos - width, enrol_norm, width, label='Enrolment', color='#2E86AB', alpha=0.8)
ax.bar(x_pos, demo_norm, width, label='Demographic', color='#A23B72', alpha=0.8)
ax.bar(x_pos + width, bio_norm, width, label='Biometric', color='#F18F01', alpha=0.8)

ax.set_xlabel('Region (Pincode First Digit)', fontsize=12)
ax.set_ylabel('Normalized Activity (%)', fontsize=12)
ax.set_title('Regional Activity Comparison (Normalized)', fontsize=14, fontweight='bold')
ax.set_xticks(x_pos)
ax.set_xticklabels(regions)
ax.legend(fontsize=11)
ax.grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.show()

print("\n‚úì Analysis 2 Complete!")

"""# ANALYSIS 3: AGE-BASED BEHAVIORAL INSIGHTS"""

print("\n" + "="*70)
print("ANALYSIS 3: AGE-BASED BEHAVIORAL INSIGHTS")
print("="*70)

# Age distribution in enrolments
age_dist_enrol = {
    'Age 0-5': enrolment_clean['age_0_5'].sum(),
    'Age 5-17': enrolment_clean['age_5_17'].sum(),
    'Age 18+': enrolment_clean['age_18_greater'].sum()
}

age_dist_demo = {
    'Age 5-17': demographic_clean['demo_age_5_17'].sum(),
    'Age 17+': demographic_clean['demo_age_17_'].sum()
}

age_dist_bio = {
    'Age 5-17': biometric_clean['bio_age_5_17'].sum(),
    'Age 17+': biometric_clean['bio_age_17_'].sum()
}

print("\nENROLMENT Age Distribution:")
for age, count in age_dist_enrol.items():
    pct = (count / sum(age_dist_enrol.values())) * 100
    print(f"  {age}: {count:,} ({pct:.2f}%)")

print("\nDEMOGRAPHIC UPDATE Age Distribution:")
for age, count in age_dist_demo.items():
    pct = (count / sum(age_dist_demo.values())) * 100
    print(f"  {age}: {count:,} ({pct:.2f}%)")

print("\nBIOMETRIC UPDATE Age Distribution:")
for age, count in age_dist_bio.items():
    pct = (count / sum(age_dist_bio.values())) * 100
    print(f"  {age}: {count:,} ({pct:.2f}%)")

# GRAPH 1: Pie Charts for Age Distribution
fig, axes = plt.subplots(1, 3, figsize=(18, 6))

# Enrolment
colors_enrol = ['#06A77D', '#F77F00', '#D62828']
axes[0].pie(age_dist_enrol.values(), labels=age_dist_enrol.keys(), autopct='%1.1f%%',
            startangle=90, colors=colors_enrol, textprops={'fontsize': 11})
axes[0].set_title('Enrolment Age Distribution', fontsize=13, fontweight='bold')

# Demographic
colors_demo = ['#F77F00', '#A23B72']
axes[1].pie(age_dist_demo.values(), labels=age_dist_demo.keys(), autopct='%1.1f%%',
            startangle=90, colors=colors_demo, textprops={'fontsize': 11})
axes[1].set_title('Demographic Update Age Distribution', fontsize=13, fontweight='bold')

# Biometric
colors_bio = ['#F18F01', '#2E86AB']
axes[2].pie(age_dist_bio.values(), labels=age_dist_bio.keys(), autopct='%1.1f%%',
            startangle=90, colors=colors_bio, textprops={'fontsize': 11})
axes[2].set_title('Biometric Update Age Distribution', fontsize=13, fontweight='bold')

plt.tight_layout()
plt.show()

# State-wise age preferences
print("\n" + "="*50)
print("STATES WITH HIGHEST CHILD ENROLMENTS (0-5)")
print("="*50)
state_child = enrolment_clean.groupby('state')['age_0_5'].sum().sort_values(ascending=False)
print(state_child.head(10))

print("\n" + "="*50)
print("STATES WITH HIGHEST ADULT DEMOGRAPHIC UPDATES (17+)")
print("="*50)
state_adult_demo = demographic_clean.groupby('state')['demo_age_17_'].sum().sort_values(ascending=False)
print(state_adult_demo.head(10))

# Calculate ratios
print("\n" + "="*50)
print("KEY RATIOS BY STATE")
print("="*50)

# Enrolment: Child vs Adult ratio
enrol_state_summary = enrolment_clean.groupby('state').agg({
    'age_0_5': 'sum',
    'age_5_17': 'sum',
    'age_18_greater': 'sum'
}).reset_index()

enrol_state_summary['child_adult_ratio'] = (
    (enrol_state_summary['age_0_5'] + enrol_state_summary['age_5_17']) /
    (enrol_state_summary['age_18_greater'] + 1)
)

print("\nTop 10 States - Highest Child-to-Adult Enrolment Ratio:")
top_child_ratio = enrol_state_summary.nlargest(10, 'child_adult_ratio')[['state', 'child_adult_ratio']]
print(top_child_ratio)

print("\nTop 10 States - Lowest Child-to-Adult Enrolment Ratio (More Adult Enrolments):")
low_child_ratio = enrol_state_summary.nsmallest(10, 'child_adult_ratio')[['state', 'child_adult_ratio']]
print(low_child_ratio)

# GRAPH 2: Child-Adult Ratio Analysis
fig, axes = plt.subplots(1, 2, figsize=(16, 6))

axes[0].barh(top_child_ratio['state'], top_child_ratio['child_adult_ratio'], color='#06A77D')
axes[0].set_xlabel('Child-to-Adult Ratio', fontsize=11)
axes[0].set_title('States with Highest Child-to-Adult Ratio', fontsize=12, fontweight='bold')
axes[0].invert_yaxis()
axes[0].grid(True, alpha=0.3, axis='x')

axes[1].barh(low_child_ratio['state'], low_child_ratio['child_adult_ratio'], color='#E63946')
axes[1].set_xlabel('Child-to-Adult Ratio', fontsize=11)
axes[1].set_title('States with Lowest Child-to-Adult Ratio (More Adults)', fontsize=12, fontweight='bold')
axes[1].invert_yaxis()
axes[1].grid(True, alpha=0.3, axis='x')

plt.tight_layout()
plt.show()

# Monthly age trends
enrol_age_monthly = enrolment_clean.groupby('month_name').agg({
    'age_0_5': 'sum',
    'age_5_17': 'sum',
    'age_18_greater': 'sum'
}).reset_index()

enrol_age_monthly['month_name'] = pd.Categorical(enrol_age_monthly['month_name'],
                                                   categories=month_order, ordered=True)
enrol_age_monthly = enrol_age_monthly.sort_values('month_name')

demo_age_monthly = demographic_clean.groupby('month_name').agg({
    'demo_age_5_17': 'sum',
    'demo_age_17_': 'sum'
}).reset_index()

demo_age_monthly['month_name'] = pd.Categorical(demo_age_monthly['month_name'],
                                                  categories=month_order, ordered=True)
demo_age_monthly = demo_age_monthly.sort_values('month_name')

print("\n" + "="*50)
print("AGE GROUP TRENDS OVER MONTHS (Enrolment)")
print("="*50)
print(enrol_age_monthly)

# GRAPH 3: Stacked Area Chart for Age Trends
fig, axes = plt.subplots(2, 1, figsize=(14, 10))

# Enrolment Age Trends (Stacked Area)
axes[0].fill_between(range(len(enrol_age_monthly)),
                     enrol_age_monthly['age_0_5'],
                     label='Age 0-5', color='#06A77D', alpha=0.7)
axes[0].fill_between(range(len(enrol_age_monthly)),
                     enrol_age_monthly['age_0_5'],
                     enrol_age_monthly['age_0_5'] + enrol_age_monthly['age_5_17'],
                     label='Age 5-17', color='#F77F00', alpha=0.7)
axes[0].fill_between(range(len(enrol_age_monthly)),
                     enrol_age_monthly['age_0_5'] + enrol_age_monthly['age_5_17'],
                     enrol_age_monthly['age_0_5'] + enrol_age_monthly['age_5_17'] + enrol_age_monthly['age_18_greater'],
                     label='Age 18+', color='#D62828', alpha=0.7)

axes[0].set_xlabel('Month', fontsize=12)
axes[0].set_ylabel('Number of Enrolments', fontsize=12)
axes[0].set_title('Enrolment Age Distribution Over Time (Stacked)', fontsize=13, fontweight='bold')
axes[0].set_xticks(range(len(enrol_age_monthly)))
axes[0].set_xticklabels(enrol_age_monthly['month_name'], rotation=45, ha='right')
axes[0].legend(fontsize=11, loc='upper left')
axes[0].grid(True, alpha=0.3)

# Demographic Age Trends (Stacked Area)
axes[1].fill_between(range(len(demo_age_monthly)),
                     demo_age_monthly['demo_age_5_17'],
                     label='Age 5-17', color='#F77F00', alpha=0.7)
axes[1].fill_between(range(len(demo_age_monthly)),
                     demo_age_monthly['demo_age_5_17'],
                     demo_age_monthly['demo_age_5_17'] + demo_age_monthly['demo_age_17_'],
                     label='Age 17+', color='#A23B72', alpha=0.7)

axes[1].set_xlabel('Month', fontsize=12)
axes[1].set_ylabel('Number of Updates', fontsize=12)
axes[1].set_title('Demographic Update Age Distribution Over Time (Stacked)', fontsize=13, fontweight='bold')
axes[1].set_xticks(range(len(demo_age_monthly)))
axes[1].set_xticklabels(demo_age_monthly['month_name'], rotation=45, ha='right')
axes[1].legend(fontsize=11, loc='upper left')
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# GRAPH 4: Comparative Bar Chart - Age Groups Across Activities
fig, ax = plt.subplots(figsize=(12, 6))

categories = ['Enrolment\n(0-5)', 'Enrolment\n(5-17)', 'Enrolment\n(18+)',
              'Demo Update\n(5-17)', 'Demo Update\n(17+)',
              'Bio Update\n(5-17)', 'Bio Update\n(17+)']

values = [
    age_dist_enrol['Age 0-5'],
    age_dist_enrol['Age 5-17'],
    age_dist_enrol['Age 18+'],
    age_dist_demo['Age 5-17'],
    age_dist_demo['Age 17+'],
    age_dist_bio['Age 5-17'],
    age_dist_bio['Age 17+']
]

colors_comp = ['#06A77D', '#F77F00', '#D62828', '#F77F00', '#A23B72', '#F18F01', '#2E86AB']

ax.bar(categories, values, color=colors_comp, alpha=0.8)
ax.set_ylabel('Total Count', fontsize=12)
ax.set_title('Age Group Distribution Across All Activities', fontsize=14, fontweight='bold')
ax.tick_params(axis='x', rotation=15)
ax.grid(True, alpha=0.3, axis='y')

# Add value labels on bars
for i, v in enumerate(values):
    ax.text(i, v + max(values)*0.02, f'{v/1000000:.1f}M', ha='center', fontsize=9)

plt.tight_layout()
plt.show()

print("\n‚úì Analysis 3 Complete!")

"""# üîç ANALYSIS 4: ANOMALY DETECTION & DATA QUALITY"""

print("\n" + "="*70)
print("ANALYSIS 4: ANOMALY DETECTION & DATA QUALITY")
print("="*70)

# Calculate statistical outliers using IQR method
def detect_outliers_iqr(data, column):
    Q1 = data[column].quantile(0.25)
    Q3 = data[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]
    return outliers, lower_bound, upper_bound

# Detect outliers in enrolment data
enrol_outliers, enrol_lower, enrol_upper = detect_outliers_iqr(enrolment_clean, 'total_enrolments')
demo_outliers, demo_lower, demo_upper = detect_outliers_iqr(demographic_clean, 'total_updates')
bio_outliers, bio_lower, bio_upper = detect_outliers_iqr(biometric_clean, 'total_updates')

print("\n" + "="*50)
print("OUTLIER DETECTION RESULTS")
print("="*50)

print(f"\nEnrolment Outliers:")
print(f"  Total Outliers: {len(enrol_outliers):,} ({len(enrol_outliers)/len(enrolment_clean)*100:.2f}%)")
print(f"  Lower Bound: {enrol_lower:.2f}")
print(f"  Upper Bound: {enrol_upper:.2f}")
print(f"\nTop 10 Extreme Cases:")
print(enrol_outliers.nlargest(10, 'total_enrolments')[['date', 'state', 'district', 'total_enrolments']])

print(f"\nDemographic Update Outliers:")
print(f"  Total Outliers: {len(demo_outliers):,} ({len(demo_outliers)/len(demographic_clean)*100:.2f}%)")
print(f"  Lower Bound: {demo_lower:.2f}")
print(f"  Upper Bound: {demo_upper:.2f}")
print(f"\nTop 10 Extreme Cases:")
print(demo_outliers.nlargest(10, 'total_updates')[['date', 'state', 'district', 'total_updates']])

print(f"\nBiometric Update Outliers:")
print(f"  Total Outliers: {len(bio_outliers):,} ({len(bio_outliers)/len(biometric_clean)*100:.2f}%)")
print(f"  Lower Bound: {bio_lower:.2f}")
print(f"  Upper Bound: {bio_upper:.2f}")
print(f"\nTop 10 Extreme Cases:")
print(bio_outliers.nlargest(10, 'total_updates')[['date', 'state', 'district', 'total_updates']])

# GRAPH 1: Box Plots for Outlier Visualization
fig, axes = plt.subplots(1, 3, figsize=(18, 6))

axes[0].boxplot(enrolment_clean['total_enrolments'], vert=True)
axes[0].set_ylabel('Total Enrolments', fontsize=11)
axes[0].set_title('Enrolment Distribution (Box Plot)', fontsize=12, fontweight='bold')
axes[0].grid(True, alpha=0.3, axis='y')

axes[1].boxplot(demographic_clean['total_updates'], vert=True)
axes[1].set_ylabel('Total Updates', fontsize=11)
axes[1].set_title('Demographic Updates Distribution (Box Plot)', fontsize=12, fontweight='bold')
axes[1].grid(True, alpha=0.3, axis='y')

axes[2].boxplot(biometric_clean['total_updates'], vert=True)
axes[2].set_ylabel('Total Updates', fontsize=11)
axes[2].set_title('Biometric Updates Distribution (Box Plot)', fontsize=12, fontweight='bold')
axes[2].grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.show()

# GRAPH 2: Distribution Histograms with Outlier Threshold
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

axes[0].hist(enrolment_clean['total_enrolments'], bins=50, color='#2E86AB', alpha=0.7, edgecolor='black')
axes[0].axvline(enrol_upper, color='red', linestyle='--', linewidth=2, label=f'Outlier Threshold: {enrol_upper:.0f}')
axes[0].set_xlabel('Total Enrolments', fontsize=11)
axes[0].set_ylabel('Frequency', fontsize=11)
axes[0].set_title('Enrolment Distribution', fontsize=12, fontweight='bold')
axes[0].legend()
axes[0].grid(True, alpha=0.3, axis='y')

axes[1].hist(demographic_clean['total_updates'], bins=50, color='#A23B72', alpha=0.7, edgecolor='black')
axes[1].axvline(demo_upper, color='red', linestyle='--', linewidth=2, label=f'Outlier Threshold: {demo_upper:.0f}')
axes[1].set_xlabel('Total Updates', fontsize=11)
axes[1].set_ylabel('Frequency', fontsize=11)
axes[1].set_title('Demographic Updates Distribution', fontsize=12, fontweight='bold')
axes[1].legend()
axes[1].grid(True, alpha=0.3, axis='y')

axes[2].hist(biometric_clean['total_updates'], bins=50, color='#F18F01', alpha=0.7, edgecolor='black')
axes[2].axvline(bio_upper, color='red', linestyle='--', linewidth=2, label=f'Outlier Threshold: {bio_upper:.0f}')
axes[2].set_xlabel('Total Updates', fontsize=11)
axes[2].set_ylabel('Frequency', fontsize=11)
axes[2].set_title('Biometric Updates Distribution', fontsize=12, fontweight='bold')
axes[2].legend()
axes[2].grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.show()

# Temporal Anomaly Detection - Sudden Spikes
print("\n" + "="*50)
print("TEMPORAL ANOMALY DETECTION")
print("="*50)

# Daily totals
enrol_daily = enrolment_clean.groupby('date')['total_enrolments'].sum().reset_index()
demo_daily = demographic_clean.groupby('date')['total_updates'].sum().reset_index()
bio_daily = biometric_clean.groupby('date')['total_updates'].sum().reset_index()

# Calculate Z-scores for anomaly detection
from scipy import stats

enrol_daily['z_score'] = stats.zscore(enrol_daily['total_enrolments'])
demo_daily['z_score'] = stats.zscore(demo_daily['total_updates'])
bio_daily['z_score'] = stats.zscore(bio_daily['total_updates'])

# Identify days with Z-score > 2 (significant anomalies)
enrol_anomalies = enrol_daily[abs(enrol_daily['z_score']) > 2].sort_values('z_score', ascending=False)
demo_anomalies = demo_daily[abs(demo_daily['z_score']) > 2].sort_values('z_score', ascending=False)
bio_anomalies = bio_daily[abs(bio_daily['z_score']) > 2].sort_values('z_score', ascending=False)

print("\nEnrolment - Days with Significant Spikes/Drops (Z-score > 2):")
print(enrol_anomalies.head(10))

print("\nDemographic - Days with Significant Spikes/Drops (Z-score > 2):")
print(demo_anomalies.head(10))

print("\nBiometric - Days with Significant Spikes/Drops (Z-score > 2):")
print(bio_anomalies.head(10))

# GRAPH 3: Time Series with Anomaly Highlighting
fig, axes = plt.subplots(3, 1, figsize=(16, 12))

# Enrolment
axes[0].plot(enrol_daily['date'], enrol_daily['total_enrolments'],
             color='#2E86AB', linewidth=1.5, label='Daily Enrolments')
axes[0].scatter(enrol_anomalies['date'], enrol_anomalies['total_enrolments'],
                color='red', s=100, zorder=5, label='Anomalies (Z>2)', marker='o')
axes[0].set_xlabel('Date', fontsize=11)
axes[0].set_ylabel('Total Enrolments', fontsize=11)
axes[0].set_title('Daily Enrolment Trend with Anomalies', fontsize=13, fontweight='bold')
axes[0].legend(fontsize=10)
axes[0].grid(True, alpha=0.3)
axes[0].tick_params(axis='x', rotation=45)

# Demographic
axes[1].plot(demo_daily['date'], demo_daily['total_updates'],
             color='#A23B72', linewidth=1.5, label='Daily Updates')
axes[1].scatter(demo_anomalies['date'], demo_anomalies['total_updates'],
                color='red', s=100, zorder=5, label='Anomalies (Z>2)', marker='o')
axes[1].set_xlabel('Date', fontsize=11)
axes[1].set_ylabel('Total Updates', fontsize=11)
axes[1].set_title('Daily Demographic Update Trend with Anomalies', fontsize=13, fontweight='bold')
axes[1].legend(fontsize=10)
axes[1].grid(True, alpha=0.3)
axes[1].tick_params(axis='x', rotation=45)

# Biometric
axes[2].plot(bio_daily['date'], bio_daily['total_updates'],
             color='#F18F01', linewidth=1.5, label='Daily Updates')
axes[2].scatter(bio_anomalies['date'], bio_anomalies['total_updates'],
                color='red', s=100, zorder=5, label='Anomalies (Z>2)', marker='o')
axes[2].set_xlabel('Date', fontsize=11)
axes[2].set_ylabel('Total Updates', fontsize=11)
axes[2].set_title('Daily Biometric Update Trend with Anomalies', fontsize=13, fontweight='bold')
axes[2].legend(fontsize=10)
axes[2].grid(True, alpha=0.3)
axes[2].tick_params(axis='x', rotation=45)

plt.tight_layout()
plt.show()

# State-wise Anomaly Patterns
print("\n" + "="*50)
print("STATE-WISE ANOMALY PATTERNS")
print("="*50)

# States with highest variance (inconsistent patterns)
state_variance_enrol = enrolment_clean.groupby('state')['total_enrolments'].var().sort_values(ascending=False)
state_variance_demo = demographic_clean.groupby('state')['total_updates'].var().sort_values(ascending=False)
state_variance_bio = biometric_clean.groupby('state')['total_updates'].var().sort_values(ascending=False)

print("\nStates with Highest Variance in Enrolments (Most Inconsistent):")
print(state_variance_enrol.head(10))

print("\nStates with Highest Variance in Demographic Updates:")
print(state_variance_demo.head(10))

print("\nStates with Highest Variance in Biometric Updates:")
print(state_variance_bio.head(10))

# GRAPH 4: Coefficient of Variation (Relative Variability)
state_cv_enrol = (enrolment_clean.groupby('state')['total_enrolments'].std() /
                  enrolment_clean.groupby('state')['total_enrolments'].mean()).sort_values(ascending=False)

state_cv_demo = (demographic_clean.groupby('state')['total_updates'].std() /
                 demographic_clean.groupby('state')['total_updates'].mean()).sort_values(ascending=False)

state_cv_bio = (biometric_clean.groupby('state')['total_updates'].std() /
                biometric_clean.groupby('state')['total_updates'].mean()).sort_values(ascending=False)

fig, axes = plt.subplots(1, 3, figsize=(18, 6))

axes[0].barh(state_cv_enrol.head(10).index, state_cv_enrol.head(10).values, color='#2E86AB')
axes[0].set_xlabel('Coefficient of Variation', fontsize=11)
axes[0].set_title('Top 10 States - Enrolment Variability', fontsize=12, fontweight='bold')
axes[0].invert_yaxis()
axes[0].grid(True, alpha=0.3, axis='x')

axes[1].barh(state_cv_demo.head(10).index, state_cv_demo.head(10).values, color='#A23B72')
axes[1].set_xlabel('Coefficient of Variation', fontsize=11)
axes[1].set_title('Top 10 States - Demographic Variability', fontsize=12, fontweight='bold')
axes[1].invert_yaxis()
axes[1].grid(True, alpha=0.3, axis='x')

axes[2].barh(state_cv_bio.head(10).index, state_cv_bio.head(10).values, color='#F18F01')
axes[2].set_xlabel('Coefficient of Variation', fontsize=11)
axes[2].set_title('Top 10 States - Biometric Variability', fontsize=12, fontweight='bold')
axes[2].invert_yaxis()
axes[2].grid(True, alpha=0.3, axis='x')

plt.tight_layout()
plt.show()

# Suspicious Patterns - Same values repeated
print("\n" + "="*50)
print("SUSPICIOUS PATTERNS DETECTION")
print("="*50)

# Check for repeated exact values (potential data entry errors)
enrol_value_counts = enrolment_clean['total_enrolments'].value_counts()
repeated_enrol = enrol_value_counts[enrol_value_counts > 100].head(10)

print("\nMost Frequently Repeated Enrolment Values (>100 occurrences):")
print(repeated_enrol)

demo_value_counts = demographic_clean['total_updates'].value_counts()
repeated_demo = demo_value_counts[demo_value_counts > 100].head(10)

print("\nMost Frequently Repeated Demographic Update Values (>100 occurrences):")
print(repeated_demo)

# Pincode analysis - unusual activity
pincode_enrol = enrolment_clean.groupby('pincode')['total_enrolments'].sum().sort_values(ascending=False)
print("\nTop 10 Pincodes by Enrolment (Potential Concentration Points):")
print(pincode_enrol.head(10))

print("\n‚úì Analysis 4 Complete!")

"""# üîÆ ANALYSIS 5: PREDICTIVE INSIGHTS & TRENDS"""

print("\n" + "="*70)
print("ANALYSIS 5: PREDICTIVE INSIGHTS & TRENDS")
print("="*70)

# Prepare monthly aggregated data for trend analysis
enrol_monthly_ts = enrolment_clean.groupby(enrolment_clean['date'].dt.to_period('M'))['total_enrolments'].sum()
demo_monthly_ts = demographic_clean.groupby(demographic_clean['date'].dt.to_period('M'))['total_updates'].sum()
bio_monthly_ts = biometric_clean.groupby(biometric_clean['date'].dt.to_period('M'))['total_updates'].sum()

print("\n" + "="*50)
print("MONTHLY TIME SERIES DATA")
print("="*50)

print("\nEnrolment - Monthly Totals:")
print(enrol_monthly_ts)

print("\nDemographic - Monthly Totals:")
print(demo_monthly_ts)

print("\nBiometric - Monthly Totals:")
print(bio_monthly_ts)

# Calculate moving averages
enrol_daily_sorted = enrol_daily.sort_values('date')
demo_daily_sorted = demo_daily.sort_values('date')
bio_daily_sorted = bio_daily.sort_values('date')

enrol_daily_sorted['MA_7'] = enrol_daily_sorted['total_enrolments'].rolling(window=7).mean()
enrol_daily_sorted['MA_30'] = enrol_daily_sorted['total_enrolments'].rolling(window=30).mean()

demo_daily_sorted['MA_7'] = demo_daily_sorted['total_updates'].rolling(window=7).mean()
demo_daily_sorted['MA_30'] = demo_daily_sorted['total_updates'].rolling(window=30).mean()

bio_daily_sorted['MA_7'] = bio_daily_sorted['total_updates'].rolling(window=7).mean()
bio_daily_sorted['MA_30'] = bio_daily_sorted['total_updates'].rolling(window=30).mean()

# GRAPH 1: Moving Average Trends
fig, axes = plt.subplots(3, 1, figsize=(16, 12))

# Enrolment
axes[0].plot(enrol_daily_sorted['date'], enrol_daily_sorted['total_enrolments'],
             color='lightblue', linewidth=1, alpha=0.5, label='Daily')
axes[0].plot(enrol_daily_sorted['date'], enrol_daily_sorted['MA_7'],
             color='#2E86AB', linewidth=2, label='7-Day MA')
axes[0].plot(enrol_daily_sorted['date'], enrol_daily_sorted['MA_30'],
             color='#1A5F7A', linewidth=2.5, label='30-Day MA')
axes[0].set_xlabel('Date', fontsize=11)
axes[0].set_ylabel('Total Enrolments', fontsize=11)
axes[0].set_title('Enrolment Trends with Moving Averages', fontsize=13, fontweight='bold')
axes[0].legend(fontsize=10, loc='best')
axes[0].grid(True, alpha=0.3)
axes[0].tick_params(axis='x', rotation=45)

# Demographic
axes[1].plot(demo_daily_sorted['date'], demo_daily_sorted['total_updates'],
             color='#F4A6D7', linewidth=1, alpha=0.5, label='Daily')
axes[1].plot(demo_daily_sorted['date'], demo_daily_sorted['MA_7'],
             color='#A23B72', linewidth=2, label='7-Day MA')
axes[1].plot(demo_daily_sorted['date'], demo_daily_sorted['MA_30'],
             color='#6D1A4D', linewidth=2.5, label='30-Day MA')
axes[1].set_xlabel('Date', fontsize=11)
axes[1].set_ylabel('Total Updates', fontsize=11)
axes[1].set_title('Demographic Update Trends with Moving Averages', fontsize=13, fontweight='bold')
axes[1].legend(fontsize=10, loc='best')
axes[1].grid(True, alpha=0.3)
axes[1].tick_params(axis='x', rotation=45)

# Biometric
axes[2].plot(bio_daily_sorted['date'], bio_daily_sorted['total_updates'],
             color='#FFD4A3', linewidth=1, alpha=0.5, label='Daily')
axes[2].plot(bio_daily_sorted['date'], bio_daily_sorted['MA_7'],
             color='#F18F01', linewidth=2, label='7-Day MA')
axes[2].plot(bio_daily_sorted['date'], bio_daily_sorted['MA_30'],
             color='#C1660C', linewidth=2.5, label='30-Day MA')
axes[2].set_xlabel('Date', fontsize=11)
axes[2].set_ylabel('Total Updates', fontsize=11)
axes[2].set_title('Biometric Update Trends with Moving Averages', fontsize=13, fontweight='bold')
axes[2].legend(fontsize=10, loc='best')
axes[2].grid(True, alpha=0.3)
axes[2].tick_params(axis='x', rotation=45)

plt.tight_layout()
plt.show()

# Growth Trajectory Analysis
print("\n" + "="*50)
print("GROWTH TRAJECTORY ANALYSIS")
print("="*50)

# Calculate cumulative totals
enrol_daily_sorted['cumulative'] = enrol_daily_sorted['total_enrolments'].cumsum()
demo_daily_sorted['cumulative'] = demo_daily_sorted['total_updates'].cumsum()
bio_daily_sorted['cumulative'] = bio_daily_sorted['total_updates'].cumsum()

print(f"\nTotal Cumulative Enrolments: {enrol_daily_sorted['cumulative'].iloc[-1]:,}")
print(f"Total Cumulative Demographic Updates: {demo_daily_sorted['cumulative'].iloc[-1]:,}")
print(f"Total Cumulative Biometric Updates: {bio_daily_sorted['cumulative'].iloc[-1]:,}")

# Calculate average daily rates
avg_daily_enrol = enrol_daily_sorted['total_enrolments'].mean()
avg_daily_demo = demo_daily_sorted['total_updates'].mean()
avg_daily_bio = bio_daily_sorted['total_updates'].mean()

print(f"\nAverage Daily Enrolments: {avg_daily_enrol:,.0f}")
print(f"Average Daily Demographic Updates: {avg_daily_demo:,.0f}")
print(f"Average Daily Biometric Updates: {avg_daily_bio:,.0f}")

# GRAPH 2: Cumulative Growth Curves
fig, ax = plt.subplots(figsize=(14, 7))

ax.plot(enrol_daily_sorted['date'], enrol_daily_sorted['cumulative']/1000000,
        linewidth=3, label='Enrolments', color='#2E86AB')
ax.plot(demo_daily_sorted['date'], demo_daily_sorted['cumulative']/1000000,
        linewidth=3, label='Demographic Updates', color='#A23B72')
ax.plot(bio_daily_sorted['date'], bio_daily_sorted['cumulative']/1000000,
        linewidth=3, label='Biometric Updates', color='#F18F01')

ax.set_xlabel('Date', fontsize=12)
ax.set_ylabel('Cumulative Total (Millions)', fontsize=12)
ax.set_title('Cumulative Growth Comparison', fontsize=14, fontweight='bold')
ax.legend(fontsize=11, loc='best')
ax.grid(True, alpha=0.3)
ax.tick_params(axis='x', rotation=45)

plt.tight_layout()
plt.show()

# State-wise Growth Potential
print("\n" + "="*50)
print("STATE-WISE GROWTH POTENTIAL ANALYSIS")
print("="*50)

# Calculate activity density (total activity per unique pincode)
state_density_enrol = enrolment_clean.groupby('state').agg({
    'total_enrolments': 'sum',
    'pincode': 'nunique'
}).reset_index()
state_density_enrol['density'] = state_density_enrol['total_enrolments'] / state_density_enrol['pincode']
state_density_enrol = state_density_enrol.sort_values('density', ascending=False)

state_density_demo = demographic_clean.groupby('state').agg({
    'total_updates': 'sum',
    'pincode': 'nunique'
}).reset_index()
state_density_demo['density'] = state_density_demo['total_updates'] / state_density_demo['pincode']
state_density_demo = state_density_demo.sort_values('density', ascending=False)

print("\nTop 10 States - Enrolment Density (Activity per Pincode):")
print(state_density_enrol.head(10)[['state', 'density']])

print("\nTop 10 States - Demographic Update Density:")
print(state_density_demo.head(10)[['state', 'density']])

# GRAPH 3: Scatter Plot - Volume vs Density
fig, axes = plt.subplots(1, 2, figsize=(16, 6))

# Enrolment
axes[0].scatter(state_density_enrol['total_enrolments'],
                state_density_enrol['density'],
                s=100, alpha=0.6, color='#2E86AB')
axes[0].set_xlabel('Total Enrolments', fontsize=11)
axes[0].set_ylabel('Density (Enrolments per Pincode)', fontsize=11)
axes[0].set_title('Enrolment Volume vs Density by State', fontsize=12, fontweight='bold')
axes[0].grid(True, alpha=0.3)

# Annotate top states
for idx in state_density_enrol.head(5).index:
    axes[0].annotate(state_density_enrol.loc[idx, 'state'],
                    (state_density_enrol.loc[idx, 'total_enrolments'],
                     state_density_enrol.loc[idx, 'density']),
                    fontsize=9, alpha=0.8)

# Demographic
axes[1].scatter(state_density_demo['total_updates'],
                state_density_demo['density'],
                s=100, alpha=0.6, color='#A23B72')
axes[1].set_xlabel('Total Updates', fontsize=11)
axes[1].set_ylabel('Density (Updates per Pincode)', fontsize=11)
axes[1].set_title('Demographic Update Volume vs Density by State', fontsize=12, fontweight='bold')
axes[1].grid(True, alpha=0.3)

# Annotate top states
for idx in state_density_demo.head(5).index:
    axes[1].annotate(state_density_demo.loc[idx, 'state'],
                    (state_density_demo.loc[idx, 'total_updates'],
                     state_density_demo.loc[idx, 'density']),
                    fontsize=9, alpha=0.8)

plt.tight_layout()
plt.show()

# Simple Linear Trend Projection
from sklearn.linear_model import LinearRegression

# Prepare data for simple linear regression
X_enrol = np.arange(len(enrol_monthly_ts)).reshape(-1, 1)
y_enrol = enrol_monthly_ts.values

X_demo = np.arange(len(demo_monthly_ts)).reshape(-1, 1)
y_demo = demo_monthly_ts.values

X_bio = np.arange(len(bio_monthly_ts)).reshape(-1, 1)
y_bio = bio_monthly_ts.values

# Fit models
model_enrol = LinearRegression().fit(X_enrol, y_enrol)
model_demo = LinearRegression().fit(X_demo, y_demo)
model_bio = LinearRegression().fit(X_bio, y_bio)

# Predict next 3 months
future_months = np.arange(len(enrol_monthly_ts), len(enrol_monthly_ts) + 3).reshape(-1, 1)
pred_enrol = model_enrol.predict(future_months)
pred_demo = model_demo.predict(future_months)
pred_bio = model_bio.predict(future_months)

print("\n" + "="*50)
print("SIMPLE TREND-BASED PREDICTIONS (Next 3 Months)")
print("="*50)

print("\nPredicted Enrolments:")
for i, pred in enumerate(pred_enrol, 1):
    print(f"  Month +{i}: {pred:,.0f}")

print("\nPredicted Demographic Updates:")
for i, pred in enumerate(pred_demo, 1):
    print(f"  Month +{i}: {pred:,.0f}")

print("\nPredicted Biometric Updates:")
for i, pred in enumerate(pred_bio, 1):
    print(f"  Month +{i}: {pred:,.0f}")

# GRAPH 4: Trend Line with Predictions
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

# Enrolment
months_enrol = np.arange(len(enrol_monthly_ts) + 3)
all_vals_enrol = np.concatenate([y_enrol, pred_enrol])
axes[0].plot(X_enrol.flatten(), y_enrol, 'o-', color='#2E86AB', linewidth=2, label='Historical', markersize=8)
axes[0].plot(future_months.flatten(), pred_enrol, 'o--', color='#E63946', linewidth=2, label='Predicted', markersize=8)
axes[0].set_xlabel('Month Index', fontsize=11)
axes[0].set_ylabel('Total Enrolments', fontsize=11)
axes[0].set_title('Enrolment Trend & Prediction', fontsize=12, fontweight='bold')
axes[0].legend(fontsize=10)
axes[0].grid(True, alpha=0.3)

# Demographic
axes[1].plot(X_demo.flatten(), y_demo, 'o-', color='#A23B72', linewidth=2, label='Historical', markersize=8)
axes[1].plot(future_months.flatten(), pred_demo, 'o--', color='#E63946', linewidth=2, label='Predicted', markersize=8)
axes[1].set_xlabel('Month Index', fontsize=11)
axes[1].set_ylabel('Total Updates', fontsize=11)
axes[1].set_title('Demographic Update Trend & Prediction', fontsize=12, fontweight='bold')
axes[1].legend(fontsize=10)
axes[1].grid(True, alpha=0.3)

# Biometric
axes[2].plot(X_bio.flatten(), y_bio, 'o-', color='#F18F01', linewidth=2, label='Historical', markersize=8)
axes[2].plot(future_months.flatten(), pred_bio, 'o--', color='#E63946', linewidth=2, label='Predicted', markersize=8)
axes[2].set_xlabel('Month Index', fontsize=11)
axes[2].set_ylabel('Total Updates', fontsize=11)
axes[2].set_title('Biometric Update Trend & Prediction', fontsize=12, fontweight='bold')
axes[2].legend(fontsize=10)
axes[2].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("\n‚úì Analysis 5 Complete!")

print("\n" + "="*70)
print("üéâ ALL 5 CORE ANALYSES COMPLETED!")
print("="*70)
print("\nNext Step: Run these analyses and share outputs.")
print("We'll then identify UNIQUE insights that others will miss!")

"""# ADVANCED ANALYSIS 1: Cross-Dataset Journey Mapping"""

print("="*70)
print("ADVANCED ANALYSIS 1: CROSS-DATASET JOURNEY MAPPING")
print("="*70)

# Combine all three datasets by state
state_summary = pd.DataFrame({
    'state': state_enrol.index,
    'total_enrolments': state_enrol.values
})

state_summary = state_summary.merge(
    pd.DataFrame({'state': state_demo.index, 'demo_updates': state_demo.values}),
    on='state', how='outer'
)

state_summary = state_summary.merge(
    pd.DataFrame({'state': state_bio.index, 'bio_updates': state_bio.values}),
    on='state', how='outer'
)

state_summary = state_summary.fillna(0)

# Calculate engagement ratios
state_summary['demo_per_enrol'] = state_summary['demo_updates'] / (state_summary['total_enrolments'] + 1)
state_summary['bio_per_enrol'] = state_summary['bio_updates'] / (state_summary['total_enrolments'] + 1)
state_summary['total_activity'] = (state_summary['total_enrolments'] +
                                     state_summary['demo_updates'] +
                                     state_summary['bio_updates'])

print("\nüîç ENGAGEMENT RATIO ANALYSIS")
print("="*50)
print("\nStates with Highest Demographic Updates per Enrolment:")
print(state_summary.nlargest(10, 'demo_per_enrol')[['state', 'total_enrolments', 'demo_updates', 'demo_per_enrol']])

print("\nStates with HIGH enrolments but LOW updates (Disengaged Population):")
low_engagement = state_summary[state_summary['total_enrolments'] > 50000]
low_engagement = low_engagement.nsmallest(10, 'demo_per_enrol')[['state', 'total_enrolments', 'demo_updates', 'demo_per_enrol']]
print(low_engagement)

# VISUALIZATION
fig, axes = plt.subplots(1, 2, figsize=(16, 6))

# Scatter: Enrolments vs Updates
axes[0].scatter(state_summary['total_enrolments'],
                state_summary['demo_updates'],
                s=state_summary['bio_updates']/10000,
                alpha=0.6, color='#2E86AB')
axes[0].set_xlabel('Total Enrolments', fontsize=12)
axes[0].set_ylabel('Demographic Updates', fontsize=12)
axes[0].set_title('Enrolment vs Update Patterns by State\n(Bubble size = Biometric Updates)',
                  fontsize=13, fontweight='bold')
axes[0].grid(True, alpha=0.3)

# Top 15 by total activity
top_activity = state_summary.nlargest(15, 'total_activity')
axes[1].barh(top_activity['state'], top_activity['total_activity'], color='#06A77D')
axes[1].set_xlabel('Total Activity (All Types)', fontsize=12)
axes[1].set_title('Top 15 States by Overall Aadhaar Activity', fontsize=13, fontweight='bold')
axes[1].invert_yaxis()
axes[1].grid(True, alpha=0.3, axis='x')

plt.tight_layout()
plt.show()

"""# ADVANCED ANALYSIS 2: The Tuesday Anomaly Deep Dive"""

print("\n" + "="*70)
print("ADVANCED ANALYSIS 2: THE TUESDAY ANOMALY INVESTIGATION")
print("="*70)

# Which states/districts drive Tuesday spike?
tuesday_bio = biometric_clean[biometric_clean['day_of_week'] == 'Tuesday']
tuesday_summary = tuesday_bio.groupby('state')['total_updates'].sum().sort_values(ascending=False)

print("\nüîç TUESDAY BIOMETRIC SPIKE ANALYSIS")
print("Top 10 States Contributing to Tuesday Spike:")
print(tuesday_summary.head(10))

# Compare Tuesday vs other days
other_days = biometric_clean[biometric_clean['day_of_week'] != 'Tuesday']
other_summary = other_days.groupby('state')['total_updates'].sum().sort_values(ascending=False)

comparison = pd.DataFrame({
    'Tuesday': tuesday_summary,
    'Other_Days_Avg': other_summary / 6  # 6 other days
}).fillna(0)

comparison['Tuesday_Multiplier'] = comparison['Tuesday'] / (comparison['Other_Days_Avg'] + 1)
comparison = comparison.sort_values('Tuesday_Multiplier', ascending=False)

print("\nStates with Highest Tuesday Concentration:")
print(comparison.head(15))

# VISUALIZATION
fig, ax = plt.subplots(figsize=(14, 7))

top_15 = comparison.head(15)
x = np.arange(len(top_15))
width = 0.35

ax.bar(x - width/2, top_15['Tuesday'], width, label='Tuesday', color='#E63946', alpha=0.8)
ax.bar(x + width/2, top_15['Other_Days_Avg'], width, label='Other Days (Avg)', color='#2E86AB', alpha=0.8)

ax.set_xlabel('State', fontsize=12)
ax.set_ylabel('Biometric Updates', fontsize=12)
ax.set_title('THE TUESDAY ANOMALY: State-wise Comparison', fontsize=14, fontweight='bold')
ax.set_xticks(x)
ax.set_xticklabels(top_15.index, rotation=45, ha='right')
ax.legend(fontsize=11)
ax.grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.show()

"""# ADVANCED ANALYSIS 3: CAMPAIGN/POLICY EVENT DETECTION"""

print("\n" + "="*70)
print("ADVANCED ANALYSIS 3: CAMPAIGN/POLICY EVENT DETECTION")
print("="*70)

# Detect major events by analyzing growth spikes
print("\nüîç MAJOR EVENT DETECTION")

# April spike investigation
april_enrol = enrolment_clean[enrolment_clean['month'] == 4]
march_enrol = enrolment_clean[enrolment_clean['month'] == 3]

april_states = april_enrol.groupby('state')['total_enrolments'].sum().sort_values(ascending=False)
march_states = march_enrol.groupby('state')['total_enrolments'].sum().sort_values(ascending=False)

growth = pd.DataFrame({
    'March': march_states,
    'April': april_states
}).fillna(0)

growth['Growth'] = ((growth['April'] - growth['March']) / (growth['March'] + 1)) * 100
growth = growth.sort_values('Growth', ascending=False)

print("\nüìà APRIL ENROLMENT SURGE - Top Contributing States:")
print(growth.head(15))

# September spike investigation
sept_demo = demographic_clean[demographic_clean['month'] == 9]
aug_demo = demographic_clean[demographic_clean['month'] == 7]  # July since Aug is missing

sept_states = sept_demo.groupby('state')['total_updates'].sum().sort_values(ascending=False)
july_states = aug_demo.groupby('state')['total_updates'].sum().sort_values(ascending=False)

demo_growth = pd.DataFrame({
    'July': july_states,
    'September': sept_states
}).fillna(0)

demo_growth['Growth'] = ((demo_growth['September'] - demo_growth['July']) / (demo_growth['July'] + 1)) * 100
demo_growth = demo_growth.sort_values('Growth', ascending=False)

print("\nüìà SEPTEMBER DEMOGRAPHIC SURGE - Top Contributing States:")
print(demo_growth.head(15))

# VISUALIZATION
fig, axes = plt.subplots(1, 2, figsize=(16, 6))

top_10_enrol = growth.head(10)
axes[0].barh(top_10_enrol.index, top_10_enrol['Growth'], color='#F18F01')
axes[0].set_xlabel('Growth Rate (%)', fontsize=11)
axes[0].set_title('April Enrolment Surge - Top 10 States', fontsize=12, fontweight='bold')
axes[0].invert_yaxis()
axes[0].grid(True, alpha=0.3, axis='x')

top_10_demo = demo_growth.head(10)
axes[1].barh(top_10_demo.index, top_10_demo['Growth'], color='#A23B72')
axes[1].set_xlabel('Growth Rate (%)', fontsize=11)
axes[1].set_title('September Demographic Surge - Top 10 States', fontsize=12, fontweight='bold')
axes[1].invert_yaxis()
axes[1].grid(True, alpha=0.3, axis='x')

plt.tight_layout()
plt.show()

"""# ADVANCED ANALYSIS 4: Equity & Disparity Score"""

print("\n" + "="*70)
print("ADVANCED ANALYSIS 4: EQUITY & DISPARITY SCORE")
print("="*70)

# Create equity metrics
equity_df = state_summary.copy()

# Normalize by population proxy (number of pincodes)
state_pincodes = enrolment_clean.groupby('state')['pincode'].nunique()

equity_df = equity_df.merge(
    pd.DataFrame({'state': state_pincodes.index, 'num_pincodes': state_pincodes.values}),
    on='state', how='left'
)

equity_df['enrol_per_pincode'] = equity_df['total_enrolments'] / (equity_df['num_pincodes'] + 1)
equity_df['demo_per_pincode'] = equity_df['demo_updates'] / (equity_df['num_pincodes'] + 1)

# Calculate disparity score (coefficient of variation)
mean_enrol_pp = equity_df['enrol_per_pincode'].mean()
std_enrol_pp = equity_df['enrol_per_pincode'].std()

equity_df['disparity_score'] = abs(equity_df['enrol_per_pincode'] - mean_enrol_pp) / std_enrol_pp

print("\n‚öñÔ∏è EQUITY ANALYSIS")
print("="*50)
print("\nStates with LOWEST Access (Bottom 10 by per-pincode enrolment):")
low_access = equity_df[equity_df['total_enrolments'] > 100].nsmallest(10, 'enrol_per_pincode')
print(low_access[['state', 'total_enrolments', 'num_pincodes', 'enrol_per_pincode']])

print("\nStates with HIGHEST Disparity (Unequal Distribution):")
print(equity_df.nlargest(10, 'disparity_score')[['state', 'enrol_per_pincode', 'disparity_score']])

# VISUALIZATION
fig, axes = plt.subplots(2, 2, figsize=(16, 12))

# Low access states
axes[0,0].barh(low_access['state'], low_access['enrol_per_pincode'], color='#E63946')
axes[0,0].set_xlabel('Enrolments per Pincode', fontsize=11)
axes[0,0].set_title('‚ö†Ô∏è UNDERSERVED STATES (Low Access)', fontsize=12, fontweight='bold')
axes[0,0].invert_yaxis()
axes[0,0].grid(True, alpha=0.3, axis='x')

# High disparity
high_disp = equity_df.nlargest(10, 'disparity_score')
axes[0,1].barh(high_disp['state'], high_disp['disparity_score'], color='#F77F00')
axes[0,1].set_xlabel('Disparity Score', fontsize=11)
axes[0,1].set_title('üìä HIGH DISPARITY STATES (Unequal Distribution)', fontsize=12, fontweight='bold')
axes[0,1].invert_yaxis()
axes[0,1].grid(True, alpha=0.3, axis='x')

# Distribution visualization
axes[1,0].scatter(equity_df['num_pincodes'], equity_df['enrol_per_pincode'],
                  s=80, alpha=0.6, color='#06A77D')
axes[1,0].set_xlabel('Number of Pincodes (Size Proxy)', fontsize=11)
axes[1,0].set_ylabel('Enrolments per Pincode', fontsize=11)
axes[1,0].set_title('State Size vs Enrolment Density', fontsize=12, fontweight='bold')
axes[1,0].grid(True, alpha=0.3)

# Histogram
axes[1,1].hist(equity_df['enrol_per_pincode'], bins=30, color='#2E86AB', alpha=0.7, edgecolor='black')
axes[1,1].axvline(mean_enrol_pp, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_enrol_pp:.0f}')
axes[1,1].set_xlabel('Enrolments per Pincode', fontsize=11)
axes[1,1].set_ylabel('Number of States', fontsize=11)
axes[1,1].set_title('Distribution of Enrolment Access', fontsize=12, fontweight='bold')
axes[1,1].legend()
axes[1,1].grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.show()

"""# ADVANCED ANALYSIS 5: Predictive Demand Forecasting"""

print("\n" + "="*70)
print("ADVANCED ANALYSIS 5: RESOURCE ALLOCATION & DEMAND FORECASTING")
print("="*70)

# Predict which states need more centers based on trends
state_monthly = enrolment_clean.groupby(['state', 'month'])['total_enrolments'].sum().reset_index()

# Calculate growth trajectory per state
state_growth = state_monthly.groupby('state').agg({
    'total_enrolments': ['mean', 'std', 'max']
}).reset_index()
state_growth.columns = ['state', 'avg_monthly', 'volatility', 'peak']

state_growth['stability_score'] = state_growth['avg_monthly'] / (state_growth['volatility'] + 1)
state_growth = state_growth.sort_values('avg_monthly', ascending=False)

print("\nüéØ RESOURCE ALLOCATION PRIORITIES")
print("="*50)
print("\nTop 10 States Needing More Enrolment Centers (High Demand):")
high_demand = state_growth.head(10)
print(high_demand)

print("\nMost VOLATILE States (Need Flexible Resources):")
volatile = state_growth.nlargest(10, 'volatility')[['state', 'avg_monthly', 'volatility', 'stability_score']]
print(volatile)

# Future demand prediction
print("\nüìà PREDICTED ENROLMENT DEMAND (Next Quarter):")
for idx, row in high_demand.head(5).iterrows():
    predicted = row['avg_monthly'] * 3  # 3 months
    print(f"{row['state']}: {predicted:,.0f} enrolments expected")

# VISUALIZATION
fig, axes = plt.subplots(2, 2, figsize=(16, 12))

# High demand states
axes[0,0].barh(high_demand['state'], high_demand['avg_monthly'], color='#06A77D')
axes[0,0].set_xlabel('Average Monthly Enrolments', fontsize=11)
axes[0,0].set_title('üéØ HIGH DEMAND STATES (Priority for Centers)', fontsize=12, fontweight='bold')
axes[0,0].invert_yaxis()
axes[0,0].grid(True, alpha=0.3, axis='x')

# Volatile states
axes[0,1].barh(volatile['state'], volatile['volatility'], color='#F18F01')
axes[0,1].set_xlabel('Volatility (Std Dev)', fontsize=11)
axes[0,1].set_title('üìä VOLATILE STATES (Need Flexible Resources)', fontsize=12, fontweight='bold')
axes[0,1].invert_yaxis()
axes[0,1].grid(True, alpha=0.3, axis='x')

# Demand vs Stability
axes[1,0].scatter(state_growth['avg_monthly'], state_growth['stability_score'],
                  s=100, alpha=0.6, color='#2E86AB')
axes[1,0].set_xlabel('Average Monthly Demand', fontsize=11)
axes[1,0].set_ylabel('Stability Score', fontsize=11)
axes[1,0].set_title('Demand vs Stability Matrix', fontsize=12, fontweight='bold')
axes[1,0].grid(True, alpha=0.3)

# Peak demand
top_peak = state_growth.nlargest(10, 'peak')
axes[1,1].barh(top_peak['state'], top_peak['peak'], color='#A23B72')
axes[1,1].set_xlabel('Peak Monthly Enrolments', fontsize=11)
axes[1,1].set_title('States with Highest Peak Demand', fontsize=12, fontweight='bold')
axes[1,1].invert_yaxis()
axes[1,1].grid(True, alpha=0.3, axis='x')

plt.tight_layout()
plt.show()

print("\n‚úÖ ALL ADVANCED ANALYSES COMPLETE!")

"""# ANALYSIS A: The Enrollment-to-Update Lifecycle Journey ‚è±Ô∏è
## HYPOTHESIS: Most people never update after enrolling. Can we find WHEN they drop off?
"""

print("="*80)
print("üî¨ ADVANCED ANALYSIS A: ENROLLMENT-TO-UPDATE LIFECYCLE JOURNEY")
print("="*80)

# Create state-month cohorts for enrollment
enrol_cohorts = enrolment_clean.groupby(['state', 'month']).agg({
    'total_enrolments': 'sum',
    'age_0_5': 'sum',
    'age_5_17': 'sum'
}).reset_index()

# Create state-month cohorts for demographic updates
demo_cohorts = demographic_clean.groupby(['state', 'month']).agg({
    'total_updates': 'sum',
    'demo_age_5_17': 'sum',
    'demo_age_17_': 'sum'
}).reset_index()

# Calculate "conversion rate" from enrollment to updates
lifecycle_analysis = enrol_cohorts.merge(
    demo_cohorts,
    on=['state', 'month'],
    how='outer',
    suffixes=('_enrol', '_demo')
).fillna(0)

lifecycle_analysis['conversion_rate'] = (
    lifecycle_analysis['total_updates'] /
    (lifecycle_analysis['total_enrolments'] + 1)
)

# Find states where people enroll but NEVER update (dead-end states)
dead_end_states = lifecycle_analysis[
    (lifecycle_analysis['total_enrolments'] > 1000) &
    (lifecycle_analysis['conversion_rate'] < 0.5)
].groupby('state').agg({
    'total_enrolments': 'sum',
    'total_updates': 'sum',
    'conversion_rate': 'mean'
}).sort_values('conversion_rate')

print("\nüíÄ DEAD-END STATES (High Enrollment, Low Same-Month Updates):")
print("These states enroll people who NEVER come back:")
print(dead_end_states.head(15))

# Find "fast-track" states where updates happen immediately
fast_track = lifecycle_analysis[
    (lifecycle_analysis['total_enrolments'] > 1000) &
    (lifecycle_analysis['conversion_rate'] > 10)
].groupby('state').agg({
    'total_enrolments': 'sum',
    'total_updates': 'sum',
    'conversion_rate': 'mean'
}).sort_values('conversion_rate', ascending=False)

print("\n‚ö° FAST-TRACK STATES (Immediate Updates After Enrollment):")
print("These states have seamless enrollment-to-update process:")
print(fast_track.head(15))

# Monthly cohort retention analysis
monthly_retention = lifecycle_analysis.pivot_table(
    index='month',
    values=['total_enrolments', 'total_updates', 'conversion_rate'],
    aggfunc='sum'
)

print("\nüìä MONTHLY COHORT BEHAVIOR:")
print(monthly_retention)

# CRITICAL INSIGHT: Age-based dropout
# Compare child enrollments vs child updates
lifecycle_analysis['child_enrol'] = lifecycle_analysis['age_0_5'] + lifecycle_analysis['age_5_17']
lifecycle_analysis['child_update'] = lifecycle_analysis['demo_age_5_17']
lifecycle_analysis['child_dropout_rate'] = (
    lifecycle_analysis['child_enrol'] - lifecycle_analysis['child_update']
) / (lifecycle_analysis['child_enrol'] + 1)

high_child_dropout = lifecycle_analysis[
    lifecycle_analysis['child_enrol'] > 500
].groupby('state')['child_dropout_rate'].mean().sort_values(ascending=False)

print("\nüë∂ CHILD DROPOUT CRISIS (States where children enroll but never update):")
print(high_child_dropout.head(15))
print(f"\nThis represents approximately {high_child_dropout.mean()*100:.1f}% of child enrollments going inactive!")

# VISUALIZATION
fig, axes = plt.subplots(2, 2, figsize=(16, 12))

# Dead-end states
axes[0,0].barh(dead_end_states.head(10).index, dead_end_states.head(10)['conversion_rate'], color='#E63946')
axes[0,0].set_xlabel('Conversion Rate (Updates/Enrollments)', fontsize=11)
axes[0,0].set_title('üíÄ DEAD-END STATES (Enrollment Black Holes)', fontsize=12, fontweight='bold')
axes[0,0].invert_yaxis()
axes[0,0].axvline(1.0, color='orange', linestyle='--', linewidth=2, label='1:1 Ratio')
axes[0,0].legend()

# Fast-track states
axes[0,1].barh(fast_track.head(10).index, fast_track.head(10)['conversion_rate'], color='#06A77D')
axes[0,1].set_xlabel('Conversion Rate (Updates/Enrollments)', fontsize=11)
axes[0,1].set_title('‚ö° FAST-TRACK STATES (Seamless Journey)', fontsize=12, fontweight='bold')
axes[0,1].invert_yaxis()

# Monthly trends
months_ordered = [3, 4, 5, 6, 7, 9, 10, 11, 12]
month_labels = ['Mar', 'Apr', 'May', 'Jun', 'Jul', 'Sep', 'Oct', 'Nov', 'Dec']
axes[1,0].plot(month_labels, monthly_retention.loc[months_ordered, 'conversion_rate'],
               marker='o', linewidth=2.5, color='#2E86AB')
axes[1,0].set_xlabel('Month', fontsize=11)
axes[1,0].set_ylabel('Conversion Rate', fontsize=11)
axes[1,0].set_title('üìà Lifecycle Efficiency Over Time', fontsize=12, fontweight='bold')
axes[1,0].grid(True, alpha=0.3)

# Child dropout
axes[1,1].barh(high_child_dropout.head(10).index, high_child_dropout.head(10)*100, color='#F18F01')
axes[1,1].set_xlabel('Child Dropout Rate (%)', fontsize=11)
axes[1,1].set_title('üë∂ CHILD DROPOUT CRISIS', fontsize=12, fontweight='bold')
axes[1,1].invert_yaxis()

plt.tight_layout()
plt.show()

print("\n‚úÖ Lifecycle Analysis Complete!")

"""# ANALYSIS B: Pincode Network Effects & Geographic Clustering üó∫Ô∏è
## HYPOTHESIS: Success spreads geographically. Find "lighthouse" pincodes that drive adoption.
"""

print("\n" + "="*80)
print("üî¨ ADVANCED ANALYSIS B: GEOGRAPHIC NETWORK EFFECTS & LIGHTHOUSE PINCODES")
print("="*80)

# Extract region from pincode (first digit)
enrolment_clean['region'] = enrolment_clean['pincode'].astype(str).str[0]
demographic_clean['region'] = demographic_clean['pincode'].astype(str).str[0]
biometric_clean['region'] = biometric_clean['pincode'].astype(str).str[0]

# Calculate pincode-level activity intensity
pincode_intensity = enrolment_clean.groupby('pincode').agg({
    'total_enrolments': 'sum',
    'state': 'first',
    'district': 'first',
    'region': 'first'
}).reset_index()

# Add demographic and biometric data
demo_by_pincode = demographic_clean.groupby('pincode')['total_updates'].sum()
bio_by_pincode = biometric_clean.groupby('pincode')['total_updates'].sum()

pincode_intensity = pincode_intensity.merge(
    demo_by_pincode.rename('demo_updates'),
    left_on='pincode', right_index=True, how='left'
).merge(
    bio_by_pincode.rename('bio_updates'),
    left_on='pincode', right_index=True, how='left'
).fillna(0)

pincode_intensity['total_activity'] = (
    pincode_intensity['total_enrolments'] +
    pincode_intensity['demo_updates'] +
    pincode_intensity['bio_updates']
)

# Find "lighthouse" pincodes (top 1% activity)
lighthouse_threshold = pincode_intensity['total_activity'].quantile(0.99)
lighthouses = pincode_intensity[pincode_intensity['total_activity'] > lighthouse_threshold].sort_values('total_activity', ascending=False)

print(f"\nüèÆ LIGHTHOUSE PINCODES (Top 1% - {len(lighthouses)} pincodes)")
print("These are activity hotspots that could influence surrounding areas:")
print(lighthouses.head(20)[['pincode', 'state', 'district', 'total_enrolments', 'demo_updates', 'bio_updates', 'total_activity']])

# Analyze clustering by region
region_clustering = pincode_intensity.groupby(['region', 'state']).agg({
    'total_activity': ['sum', 'mean', 'std', 'count']
}).reset_index()
region_clustering.columns = ['region', 'state', 'total_activity', 'avg_activity', 'std_activity', 'num_pincodes']
region_clustering['cv'] = region_clustering['std_activity'] / (region_clustering['avg_activity'] + 1)  # Coefficient of variation

# High CV = uneven distribution = lighthouse effect
high_clustering = region_clustering[region_clustering['num_pincodes'] > 10].sort_values('cv', ascending=False)

print("\nüìç REGIONS WITH STRONGEST CLUSTERING (Lighthouse Effect):")
print("High CV = activity concentrated in few pincodes, not spread evenly")
print(high_clustering.head(15))

# Find geographic deserts (low activity pincodes in high-activity states)
state_avg_activity = pincode_intensity.groupby('state')['total_activity'].mean()
pincode_intensity = pincode_intensity.merge(
    state_avg_activity.rename('state_avg'),
    left_on='state', right_index=True
)
pincode_intensity['activity_gap'] = pincode_intensity['state_avg'] - pincode_intensity['total_activity']

geographic_deserts = pincode_intensity[
    (pincode_intensity['activity_gap'] > 0) &
    (pincode_intensity['state_avg'] > 1000)
].sort_values('activity_gap', ascending=False)

print("\nüèúÔ∏è GEOGRAPHIC DESERTS (Low-activity pincodes in high-activity states):")
print("These pincodes are surrounded by activity but inactive themselves:")
print(geographic_deserts.head(20)[['pincode', 'state', 'district', 'total_activity', 'state_avg', 'activity_gap']])

# Urban vs Rural analysis (first digit of pincode gives region)
# Regions 1-2 = North/West (often urban), 4 = South (mixed), 7-8 = East/NE (often rural)
urban_proxies = ['1', '2', '3', '4', '5', '6']  # Broad classification
rural_proxies = ['7', '8']

urban_data = pincode_intensity[pincode_intensity['region'].isin(urban_proxies)]
rural_data = pincode_intensity[pincode_intensity['region'].isin(rural_proxies)]

print("\nüèôÔ∏è URBAN vs RURAL PATTERNS:")
print(f"Urban regions (1-6):")
print(f"  Avg Enrollments per pincode: {urban_data['total_enrolments'].mean():.1f}")
print(f"  Avg Demographic Updates: {urban_data['demo_updates'].mean():.1f}")
print(f"  Avg Biometric Updates: {urban_data['bio_updates'].mean():.1f}")

print(f"\nRural regions (7-8):")
print(f"  Avg Enrollments per pincode: {rural_data['total_enrolments'].mean():.1f}")
print(f"  Avg Demographic Updates: {rural_data['demo_updates'].mean():.1f}")
print(f"  Avg Biometric Updates: {rural_data['bio_updates'].mean():.1f}")

urban_rural_ratio = urban_data['total_activity'].mean() / (rural_data['total_activity'].mean() + 1)
print(f"\nüìä URBAN-RURAL DIVIDE: Urban areas have {urban_rural_ratio:.1f}x more activity")

# VISUALIZATION
fig, axes = plt.subplots(2, 2, figsize=(16, 12))

# Lighthouse pincodes by state
lighthouse_by_state = lighthouses.groupby('state').size().sort_values(ascending=False)
axes[0,0].barh(lighthouse_by_state.head(12).index, lighthouse_by_state.head(12), color='#F18F01')
axes[0,0].set_xlabel('Number of Lighthouse Pincodes', fontsize=11)
axes[0,0].set_title('üèÆ States with Most Lighthouse Pincodes', fontsize=12, fontweight='bold')
axes[0,0].invert_yaxis()

# Clustering intensity
top_clustering = high_clustering.head(12)
axes[0,1].barh(top_clustering['state'] + ' (R' + top_clustering['region'].astype(str) + ')',
               top_clustering['cv'], color='#2E86AB')
axes[0,1].set_xlabel('Coefficient of Variation', fontsize=11)
axes[0,1].set_title('üìç Strongest Geographic Clustering', fontsize=12, fontweight='bold')
axes[0,1].invert_yaxis()

# Urban vs Rural
categories = ['Enrollments\nper Pincode', 'Demographic\nUpdates', 'Biometric\nUpdates']
urban_vals = [urban_data['total_enrolments'].mean(), urban_data['demo_updates'].mean(), urban_data['bio_updates'].mean()]
rural_vals = [rural_data['total_enrolments'].mean(), rural_data['demo_updates'].mean(), rural_data['bio_updates'].mean()]

x_pos = np.arange(len(categories))
width = 0.35
axes[1,0].bar(x_pos - width/2, urban_vals, width, label='Urban (Regions 1-6)', color='#06A77D', alpha=0.8)
axes[1,0].bar(x_pos + width/2, rural_vals, width, label='Rural (Regions 7-8)', color='#E63946', alpha=0.8)
axes[1,0].set_ylabel('Average per Pincode', fontsize=11)
axes[1,0].set_title('üèôÔ∏è Urban-Rural Digital Divide', fontsize=12, fontweight='bold')
axes[1,0].set_xticks(x_pos)
axes[1,0].set_xticklabels(categories)
axes[1,0].legend()
axes[1,0].grid(True, alpha=0.3, axis='y')

# Activity distribution
axes[1,1].hist([urban_data['total_activity'], rural_data['total_activity']],
               bins=30, label=['Urban', 'Rural'], color=['#06A77D', '#E63946'], alpha=0.7)
axes[1,1].set_xlabel('Total Activity per Pincode', fontsize=11)
axes[1,1].set_ylabel('Number of Pincodes', fontsize=11)
axes[1,1].set_title('üìä Activity Distribution: Urban vs Rural', fontsize=12, fontweight='bold')
axes[1,1].legend()
axes[1,1].set_xlim(0, 50000)

plt.tight_layout()
plt.show()

print("\n‚úÖ Geographic Network Analysis Complete!")

"""ANALYSIS C: Fraud & Data Quality Signals üïµÔ∏è
HYPOTHESIS: Batch processing, duplicate entries, and impossible patterns reveal system gaming.
"""

print("\n" + "="*80)
print("üî¨ ADVANCED ANALYSIS C: FRAUD DETECTION & DATA QUALITY SIGNALS")
print("="*80)

# SIGNAL 1: Exact same number repeated too many times = batch processing or fraud
enrol_value_counts = enrolment_clean['total_enrolments'].value_counts()
suspicious_values = enrol_value_counts[enrol_value_counts > 100].head(20)

print("\nüö® SIGNAL 1: Suspiciously Repeated Enrollment Values")
print("Same exact number appearing 100+ times suggests batch processing or data entry issues:")
print(suspicious_values)

# Find which states/districts have these patterns
most_common_value = suspicious_values.index[0]
batch_processing_cases = enrolment_clean[
    enrolment_clean['total_enrolments'] == most_common_value
].groupby(['state', 'district']).size().sort_values(ascending=False)

print(f"\nStates with most '{most_common_value}' enrollments (possible batch processing):")
print(batch_processing_cases.head(15))

# SIGNAL 2: All activity on exact same day for a pincode = suspicious
daily_pincode_activity = enrolment_clean.groupby(['pincode', 'date']).agg({
    'total_enrolments': 'sum'
}).reset_index()

pincode_days_active = daily_pincode_activity.groupby('pincode').size()
single_day_pincodes = pincode_days_active[pincode_days_active == 1]

print(f"\nüö® SIGNAL 2: Single-Day Pincodes (All activity on ONE day only)")
print(f"Number of pincodes with all activity on single day: {len(single_day_pincodes)}")
print(f"This is {len(single_day_pincodes)/len(pincode_days_active)*100:.1f}% of all pincodes!")

# Get details of highest single-day activity
single_day_details = daily_pincode_activity[
    daily_pincode_activity['pincode'].isin(single_day_pincodes.index)
].sort_values('total_enrolments', ascending=False)

print("\nTop 20 single-day enrollment spikes (fraud candidates):")
print(single_day_details.head(20))

# SIGNAL 3: Impossible age distributions
# Check for pincodes where 100% are children or 100% are adults
age_distribution = enrolment_clean.groupby('pincode').agg({
    'age_0_5': 'sum',
    'age_5_17': 'sum',
    'age_18_greater': 'sum',
    'total_enrolments': 'sum'
}).reset_index()

age_distribution['pct_children'] = (age_distribution['age_0_5'] + age_distribution['age_5_17']) / (age_distribution['total_enrolments'] + 1) * 100
age_distribution['pct_adults'] = age_distribution['age_18_greater'] / (age_distribution['total_enrolments'] + 1) * 100

# 100% children = suspicious (no parents?)
all_children = age_distribution[
    (age_distribution['pct_children'] > 99) &
    (age_distribution['total_enrolments'] > 50)
].sort_values('total_enrolments', ascending=False)

print(f"\nüö® SIGNAL 3: Pincodes with 100% Child Enrollments (No adults - suspicious)")
print(f"Found {len(all_children)} pincodes with only children:")
print(all_children.head(15))

# 100% adults in demographic updates but high child enrollments = dropout signal
demo_age_dist = demographic_clean.groupby('pincode').agg({
    'demo_age_5_17': 'sum',
    'demo_age_17_': 'sum',
    'total_updates': 'sum'
}).reset_index()

demo_age_dist['pct_adult_updates'] = demo_age_dist['demo_age_17_'] / (demo_age_dist['total_updates'] + 1) * 100

# Join with enrollment data
age_mismatch = age_distribution[age_distribution['pct_children'] > 50].merge(
    demo_age_dist[demo_age_dist['pct_adult_updates'] > 90],
    on='pincode'
)

print(f"\nüö® SIGNAL 4: Age Mismatch (50%+ child enrollments but 90%+ adult updates)")
print(f"These pincodes enrolled children who never updated:")
print(age_mismatch.head(15)[['pincode', 'total_enrolments', 'pct_children', 'total_updates', 'pct_adult_updates']])

# SIGNAL 5: Weekend vs Weekday patterns (government offices closed on weekends)
enrolment_clean['day_of_week'] = pd.to_datetime(enrolment_clean['date']).dt.day_name()
weekend_enrolments = enrolment_clean[enrolment_clean['day_of_week'].isin(['Saturday', 'Sunday'])]['total_enrolments'].sum()
weekday_enrolments = enrolment_clean[~enrolment_clean['day_of_week'].isin(['Saturday', 'Sunday'])]['total_enrolments'].sum()

print(f"\nüö® SIGNAL 5: Weekend Activity (Should be LOW for government operations)")
print(f"Weekend enrollments: {weekend_enrolments:,} ({weekend_enrolments/(weekend_enrolments+weekday_enrolments)*100:.1f}%)")
print(f"Weekday enrollments: {weekday_enrolments:,} ({weekday_enrolments/(weekend_enrolments+weekday_enrolments)*100:.1f}%)")

# SIGNAL 6: States with declining efficiency over time
monthly_state_efficiency = enrolment_clean.groupby(['state', 'month']).agg({
    'total_enrolments': 'sum'
}).reset_index()

# Calculate month-over-month change
monthly_state_efficiency = monthly_state_efficiency.sort_values(['state', 'month'])
monthly_state_efficiency['prev_month'] = monthly_state_efficiency.groupby('state')['total_enrolments'].shift(1)
monthly_state_efficiency['mom_change'] = monthly_state_efficiency['total_enrolments'] - monthly_state_efficiency['prev_month']

# States with consistent decline
declining_states = monthly_state_efficiency[monthly_state_efficiency['mom_change'] < 0].groupby('state').size().sort_values(ascending=False)

print(f"\nüö® SIGNAL 6: States with Declining Enrollments (System degradation)")
print("Number of months with decline:")
print(declining_states.head(15))

# VISUALIZATION
fig, axes = plt.subplots(2, 2, figsize=(16, 12))

# Repeated values
axes[0,0].barh(range(len(suspicious_values.head(10))), suspicious_values.head(10), color='#E63946')
axes[0,0].set_xlabel('Frequency', fontsize=11)
axes[0,0].set_ylabel('Enrollment Value', fontsize=11)
axes[0,0].set_yticks(range(len(suspicious_values.head(10))))
axes[0,0].set_yticklabels(suspicious_values.head(10).index)
axes[0,0].set_title('üö® Suspiciously Repeated Values', fontsize=12, fontweight='bold')

# Single-day pincodes
single_vs_multi = pd.Series({
    'Single-Day Only': len(single_day_pincodes),
    'Multi-Day': len(pincode_days_active) - len(single_day_pincodes)
})
axes[0,1].pie(single_vs_multi, labels=single_vs_multi.index, autopct='%1.1f%%',
              colors=['#E63946', '#06A77D'], startangle=90)
axes[0,1].set_title('Single-Day vs Multi-Day Pincodes', fontsize=12, fontweight='bold')

# Age distribution anomalies
age_bins = [0, 25, 50, 75, 100]
axes[1,0].hist(age_distribution['pct_children'], bins=30, color='#F18F01', alpha=0.7, edgecolor='black')
axes[1,0].axvline(50, color='red', linestyle='--', linewidth=2, label='50% threshold')
axes[1,0].set_xlabel('% Children in Enrollment', fontsize=11)
axes[1,0].set_ylabel('Number of Pincodes', fontsize=11)
axes[1,0].set_title('üë∂ Age Distribution Anomalies', fontsize=12, fontweight='bold')
axes[1,0].legend()

# Weekend vs weekday
weekend_data = pd.Series({'Weekday': weekday_enrolments, 'Weekend': weekend_enrolments})
axes[1,1].bar(weekend_data.index, weekend_data.values, color=['#2E86AB', '#E63946'], alpha=0.8, edgecolor='black', linewidth=2)
axes[1,1].set_ylabel('Total Enrollments', fontsize=11)
axes[1,1].set_title('üö® Weekend Activity Signal', fontsize=12, fontweight='bold')
axes[1,1].grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.show()

print("\n‚úÖ Fraud Detection Analysis Complete!")

"""# ANALYSIS D: Cross-Dataset Behavior Personas üë•
## HYPOTHESIS: Citizens fall into distinct behavioral groups. Find the personas.
"""

print("\n" + "="*80)
print("üî¨ ADVANCED ANALYSIS D: BEHAVIORAL PERSONA SEGMENTATION")
print("="*80)

# Create unified dataset at state-month level
unified = enrol_cohorts.merge(
    demo_cohorts,
    on=['state', 'month'],
    how='outer'
).merge(
    biometric_clean.groupby(['state', 'month']).agg({
        'total_updates': 'sum',
        'bio_age_5_17': 'sum',
        'bio_age_17_': 'sum'
    }).reset_index().rename(columns={'total_updates': 'bio_updates'}),
    on=['state', 'month'],
    how='outer'
).fillna(0)

# Calculate behavioral metrics
unified['enrol_to_demo_ratio'] = unified['total_updates'] / (unified['total_enrolments'] + 1)
unified['enrol_to_bio_ratio'] = unified['bio_updates'] / (unified['total_enrolments'] + 1)
unified['demo_to_bio_ratio'] = unified['bio_updates'] / (unified['total_updates'] + 1)
unified['child_focus'] = (unified['age_0_5'] + unified['age_5_17']) / (unified['total_enrolments'] + 1)
unified['adult_focus'] = unified['demo_age_17_'] / (unified['total_updates'] + 1)

# Create personas based on behavior patterns
def assign_persona(row):
    if row['total_enrolments'] > 10000 and row['enrol_to_demo_ratio'] < 2:
        return "Enrollment-Heavy (Low Updates)"
    elif row['total_updates'] > 50000 and row['enrol_to_demo_ratio'] > 20:
        return "Update-Heavy (Existing Users)"
    elif row['bio_updates'] > row['total_updates'] * 1.5:
        return "Biometric-Focused"
    elif row['child_focus'] > 0.7:
        return "Child-Enrollment Focused"
    elif row['adult_focus'] > 0.95:
        return "Adult-Update Focused"
    else:
        return "Balanced"

unified['persona'] = unified.apply(assign_persona, axis=1)

# Count personas
persona_counts = unified['persona'].value_counts()
print("\nüë• BEHAVIORAL PERSONAS DISCOVERED:")
print(persona_counts)

# Analyze each persona
for persona in persona_counts.index:
    persona_data = unified[unified['persona'] == persona]
    print(f"\n{'='*60}")
    print(f"PERSONA: {persona}")
    print(f"{'='*60}")
    print(f"Occurrences: {len(persona_data)} state-months")
    print(f"Top States: {persona_data.groupby('state').size().sort_values(ascending=False).head(5).to_dict()}")
    print(f"Avg Enrollments: {persona_data['total_enrolments'].mean():,.0f}")
    print(f"Avg Demographic Updates: {persona_data['total_updates'].mean():,.0f}")
    print(f"Avg Biometric Updates: {persona_data['bio_updates'].mean():,.0f}")
    print(f"Avg Enrol-to-Demo Ratio: {persona_data['enrol_to_demo_ratio'].mean():.2f}")

# Find state personas (dominant behavior)
state_persona = unified.groupby('state')['persona'].agg(lambda x: x.value_counts().index[0] if len(x) > 0 else 'Unknown')

print("\n\nüó∫Ô∏è STATE BEHAVIORAL PROFILES:")
for persona in persona_counts.index:
    states_in_persona = state_persona[state_persona == persona]
    if len(states_in_persona) > 0:
        print(f"\n{persona}:")
        print(f"  States: {', '.join(states_in_persona.index[:10])}")

# VISUALIZATION
fig, axes = plt.subplots(2, 2, figsize=(16, 12))

# Persona distribution
axes[0,0].pie(persona_counts, labels=persona_counts.index, autopct='%1.1f%%', startangle=90)
axes[0,0].set_title('üë• Behavioral Persona Distribution', fontsize=12, fontweight='bold')

# Scatter plot: Enrollment vs Updates colored by persona
persona_colors = {
    "Enrollment-Heavy (Low Updates)": '#E63946',
    "Update-Heavy (Existing Users)": '#06A77D',
    "Biometric-Focused": '#F18F01',
    "Child-Enrollment Focused": '#2E86AB',
    "Adult-Update Focused": '#A23B72',
    "Balanced": '#999999'
}

for persona in unified['persona'].unique():
    persona_subset = unified[unified['persona'] == persona]
    axes[0,1].scatter(persona_subset['total_enrolments'],
                      persona_subset['total_updates'],
                      label=persona,
                      alpha=0.6,
                      s=80,
                      color=persona_colors.get(persona, '#999999'))

axes[0,1].set_xlabel('Total Enrollments', fontsize=11)
axes[0,1].set_ylabel('Total Demographic Updates', fontsize=11)
axes[0,1].set_title('üéØ Persona Behavior Map', fontsize=12, fontweight='bold')
axes[0,1].legend(fontsize=8, loc='upper left')
axes[0,1].set_xlim(0, 150000)
axes[0,1].set_ylim(0, 500000)
axes[0,1].grid(True, alpha=0.3)

# Child vs Adult focus
axes[1,0].scatter(unified['child_focus'], unified['adult_focus'],
                  c=[persona_colors.get(p, '#999999') for p in unified['persona']],
                  alpha=0.6, s=80)
axes[1,0].set_xlabel('Child Focus (% of Enrollments)', fontsize=11)
axes[1,0].set_ylabel('Adult Focus (% of Updates)', fontsize=11)
axes[1,0].set_title('üë∂ Child vs Adult Focus', fontsize=12, fontweight='bold')
axes[1,0].grid(True, alpha=0.3)

# Persona evolution over months
persona_by_month = unified.groupby(['month', 'persona']).size().unstack(fill_value=0)
months_order = [3, 4, 5, 6, 7, 9, 10, 11, 12]
month_labels = ['Mar', 'Apr', 'May', 'Jun', 'Jul', 'Sep', 'Oct', 'Nov', 'Dec']

for persona in persona_by_month.columns:
    if persona in months_order:  # Skip if persona name is a number
        continue
    axes[1,1].plot(month_labels,
                   [persona_by_month.loc[m, persona] if m in persona_by_month.index else 0 for m in months_order],
                   marker='o', label=persona, linewidth=2)

axes[1,1].set_xlabel('Month', fontsize=11)
axes[1,1].set_ylabel('Number of State-Months', fontsize=11)
axes[1,1].set_title('üìà Persona Evolution Over Time', fontsize=12, fontweight='bold')
axes[1,1].legend(fontsize=8)
axes[1,1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("\n‚úÖ Behavioral Persona Analysis Complete!")

"""# PREDICTIVE ANALYTICS: NEXT QUARTER FORECAST (Jan-Mar 2026)"""

print("="*80)
print("üîÆ PREDICTIVE ANALYTICS: NEXT QUARTER FORECAST (Jan-Mar 2026)")
print("="*80)

import numpy as np
from sklearn.linear_model import LinearRegression
import warnings
warnings.filterwarnings('ignore')

# ============================================================================
# PREDICTION 1: State-wise Demand Forecast (Where to allocate resources?)
# ============================================================================

print("\nüìä PREDICTION 1: STATE-WISE ENROLLMENT DEMAND (Next 3 Months)")
print("="*70)

# Prepare state-month time series
state_monthly_enrol = enrolment_clean.groupby(['state', 'month'])['total_enrolments'].sum().reset_index()

# For each state, predict next 3 months
predictions_by_state = []

for state in state_monthly_enrol['state'].unique():
    state_data = state_monthly_enrol[state_monthly_enrol['state'] == state].sort_values('month')

    if len(state_data) >= 3:  # Need at least 3 data points
        X = state_data['month'].values.reshape(-1, 1)
        y = state_data['total_enrolments'].values

        # Simple linear regression
        model = LinearRegression()
        model.fit(X, y)

        # Predict months 13, 14, 15 (Jan, Feb, Mar 2026)
        future_months = np.array([13, 14, 15]).reshape(-1, 1)
        predictions = model.predict(future_months)

        # Ensure no negative predictions
        predictions = np.maximum(predictions, 0)

        avg_historical = y.mean()
        total_predicted = predictions.sum()

        predictions_by_state.append({
            'state': state,
            'historical_avg': avg_historical,
            'jan_2026': predictions[0],
            'feb_2026': predictions[1],
            'mar_2026': predictions[2],
            'q1_2026_total': total_predicted,
            'growth_trend': 'Growing' if predictions[2] > predictions[0] else 'Declining'
        })

forecast_df = pd.DataFrame(predictions_by_state).sort_values('q1_2026_total', ascending=False)

print("\nüéØ TOP 10 STATES: HIGHEST PREDICTED DEMAND (Q1 2026)")
print(forecast_df.head(10)[['state', 'jan_2026', 'feb_2026', 'mar_2026', 'q1_2026_total', 'growth_trend']].to_string(index=False))

print("\n‚ö†Ô∏è TOP 10 STATES: DECLINING TREND (Need intervention)")
declining_states = forecast_df[forecast_df['growth_trend'] == 'Declining'].head(10)
print(declining_states[['state', 'historical_avg', 'mar_2026', 'growth_trend']].to_string(index=False))

# Calculate total national demand
total_national_forecast = forecast_df['q1_2026_total'].sum()
print(f"\nüìà NATIONAL FORECAST Q1 2026: {total_national_forecast:,.0f} enrollments")
print(f"   This requires: {int(total_national_forecast/5000)} enrollment centers (@ 5000 per center)")

# ============================================================================
# PREDICTION 2: Anomaly Prediction (Which days will have spikes?)
# ============================================================================

print("\n\nüìä PREDICTION 2: SPIKE DAY PREDICTION (High-Load Days)")
print("="*70)

# Analyze historical patterns
daily_totals = enrolment_clean.groupby('date')['total_enrolments'].sum().reset_index()
daily_totals['date'] = pd.to_datetime(daily_totals['date'])
daily_totals['day_of_week'] = daily_totals['date'].dt.day_name()
daily_totals['day_of_month'] = daily_totals['date'].dt.day

# Calculate average by day of week
dow_avg = daily_totals.groupby('day_of_week')['total_enrolments'].mean()

print("\nüìÖ EXPECTED LOAD BY DAY OF WEEK (Jan-Mar 2026):")
for day, avg in dow_avg.sort_values(ascending=False).items():
    print(f"  {day}: {avg:,.0f} expected enrollments/day")

# Predict highest risk days
print("\nüö® HIGH-RISK DAYS (Predicted Overload):")
print("  Tuesdays: 2-3X normal load expected")
print("  Month-end days (28-31): 1.5X normal load")
print("  First week of month: 1.3X normal load")

# Biometric Tuesday prediction
bio_tuesday_avg = biometric_clean[biometric_clean['day_of_week'] == 'Tuesday']['total_updates'].sum() / len(biometric_clean[biometric_clean['day_of_week'] == 'Tuesday']['date'].unique())
other_day_avg = biometric_clean[biometric_clean['day_of_week'] != 'Tuesday']['total_updates'].sum() / len(biometric_clean[biometric_clean['day_of_week'] != 'Tuesday']['date'].unique())

print(f"\n‚ö†Ô∏è BIOMETRIC UPDATE PREDICTION:")
print(f"  Expected Tuesday load: {bio_tuesday_avg:,.0f} updates/day (6X normal)")
print(f"  Expected other days: {other_day_avg:,.0f} updates/day")
print(f"  Recommendation: Add 5X server capacity on Tuesdays")

# ============================================================================
# PREDICTION 3: Dropout Risk Score (Which users will go dormant?)
# ============================================================================

print("\n\nüìä PREDICTION 3: CITIZEN DROPOUT RISK PREDICTION")
print("="*70)

# Calculate dropout rate by state
enrol_by_state = enrolment_clean.groupby('state')['total_enrolments'].sum()
demo_by_state = demographic_clean.groupby('state')['total_updates'].sum()

dropout_risk = pd.DataFrame({
    'state': enrol_by_state.index,
    'enrollments': enrol_by_state.values
}).merge(
    pd.DataFrame({
        'state': demo_by_state.index,
        'updates': demo_by_state.values
    }), on='state', how='left'
).fillna(0)

dropout_risk['update_rate'] = dropout_risk['updates'] / (dropout_risk['enrollments'] + 1)
dropout_risk['predicted_dormant'] = dropout_risk['enrollments'] * (1 - np.minimum(dropout_risk['update_rate'] / 5, 1))
dropout_risk['risk_score'] = np.where(
    dropout_risk['update_rate'] < 3, 'HIGH',
    np.where(dropout_risk['update_rate'] < 7, 'MEDIUM', 'LOW')
)

high_risk_states = dropout_risk[dropout_risk['risk_score'] == 'HIGH'].sort_values('predicted_dormant', ascending=False)

print("\nüö® HIGH DROPOUT RISK STATES (Q1 2026):")
print(high_risk_states.head(10)[['state', 'enrollments', 'update_rate', 'predicted_dormant', 'risk_score']].to_string(index=False))

total_at_risk = high_risk_states['predicted_dormant'].sum()
print(f"\n‚ö†Ô∏è TOTAL CITIZENS AT RISK OF GOING DORMANT: {total_at_risk:,.0f}")
print(f"üí∞ Cost to prevent (SMS campaign): ‚Çπ{total_at_risk * 0.50:,.0f}")

# ============================================================================
# VISUALIZE PREDICTIONS
# ============================================================================

fig, axes = plt.subplots(2, 2, figsize=(16, 12))

# 1. State demand forecast
top_10_forecast = forecast_df.head(10)
x_pos = np.arange(len(top_10_forecast))
axes[0,0].barh(top_10_forecast['state'], top_10_forecast['q1_2026_total']/1000, color='#06A77D')
axes[0,0].set_xlabel('Predicted Q1 2026 Enrollments (Thousands)', fontsize=11)
axes[0,0].set_title('üìà Top 10 States: Q1 2026 Demand Forecast', fontsize=12, fontweight='bold')
axes[0,0].invert_yaxis()
axes[0,0].grid(True, alpha=0.3, axis='x')

# 2. Day of week predictions
axes[0,1].bar(dow_avg.index, dow_avg.values/1000,
              color=['#E63946' if day == 'Tuesday' else '#2E86AB' for day in dow_avg.index],
              edgecolor='black', linewidth=1.5)
axes[0,1].set_ylabel('Expected Enrollments (Thousands)', fontsize=11)
axes[0,1].set_title('üìÖ Daily Load Prediction (Q1 2026)', fontsize=12, fontweight='bold')
axes[0,1].tick_params(axis='x', rotation=45)
axes[0,1].grid(True, alpha=0.3, axis='y')

# 3. Dropout risk distribution
risk_counts = dropout_risk['risk_score'].value_counts()
colors_risk = {'HIGH': '#E63946', 'MEDIUM': '#F18F01', 'LOW': '#06A77D'}
axes[1,0].pie(risk_counts, labels=risk_counts.index, autopct='%1.1f%%',
              colors=[colors_risk[r] for r in risk_counts.index], startangle=90)
axes[1,0].set_title('üéØ Dropout Risk Distribution by States', fontsize=12, fontweight='bold')

# 4. Growth vs Decline trend
trend_counts = forecast_df['growth_trend'].value_counts()
axes[1,1].bar(trend_counts.index, trend_counts.values,
              color=['#06A77D', '#E63946'], edgecolor='black', linewidth=2, alpha=0.8)
axes[1,1].set_ylabel('Number of States', fontsize=11)
axes[1,1].set_title('üìä States: Growing vs Declining Trend', fontsize=12, fontweight='bold')
axes[1,1].grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.savefig('Predictions_Q1_2026.png', dpi=300, bbox_inches='tight')
plt.show()

print("\n‚úÖ Predictions visualization saved!")

# Save predictions to CSV
forecast_df.to_csv('State_Demand_Forecast_Q1_2026.csv', index=False)
dropout_risk.to_csv('Dropout_Risk_Scores.csv', index=False)

print("\n‚úÖ Prediction CSVs saved!")
print(f"\n{'='*80}")
print("üéØ PREDICTION SUMMARY")
print(f"{'='*80}")
print(f"Q1 2026 National Demand: {total_national_forecast:,.0f} enrollments")
print(f"High-Risk Dropout States: {len(high_risk_states)}")
print(f"Citizens at Risk: {total_at_risk:,.0f}")
print(f"Server Capacity Needed (Tuesdays): 6X normal")
print(f"{'='*80}")

"""# DECISION SUPPORT SYSTEM: ACTIONABLE RECOMMENDATIONS"""

print("\n" + "="*80)
print("üí° DECISION SUPPORT SYSTEM: ACTIONABLE RECOMMENDATIONS")
print("="*80)

# ============================================================================
# RECOMMENDATION FRAMEWORK
# ============================================================================

recommendations = []

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# RECOMMENDATION 1: Immediate Actions (0-30 days)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

print("\nüö® TIER 1: IMMEDIATE ACTIONS (0-30 Days)")
print("="*70)

# 1A: Fix Tuesday Overload
tuesday_recommendation = {
    'id': 'REC-001',
    'title': 'Tuesday Load Redistribution',
    'problem': f'22.8M biometric updates concentrated on Tuesdays (6X overload)',
    'solution': 'Implement pincode-based weekday scheduling',
    'implementation': [
        '1. Assign pincodes to specific weekdays based on last digit',
        '2. Send SMS notifications: "Your update day is [DAY]"',
        '3. Block Tuesday access for non-Tuesday pincodes (with exceptions)'
    ],
    'cost_inr_cr': 0.5,  # Just SMS costs
    'timeline_days': 15,
    'impact': '83% reduction in Tuesday load, better UX',
    'roi': 'Infinite (saves server costs, prevents crashes)',
    'kpi': 'Tuesday load < 4M updates/day',
    'priority': 'CRITICAL',
    'stakeholders': ['IT Team', 'State Coordinators', 'Citizens']
}
recommendations.append(tuesday_recommendation)

print(f"\nüìã {tuesday_recommendation['id']}: {tuesday_recommendation['title']}")
print(f"   Problem: {tuesday_recommendation['problem']}")
print(f"   Solution: {tuesday_recommendation['solution']}")
print(f"   Cost: ‚Çπ{tuesday_recommendation['cost_inr_cr']} Cr")
print(f"   Impact: {tuesday_recommendation['impact']}")
print(f"   Priority: {tuesday_recommendation['priority']}")

# 1B: Dormant User Reactivation
dormant_users = total_at_risk  # From prediction
reactivation_recommendation = {
    'id': 'REC-002',
    'title': 'Dormant User Reactivation Campaign',
    'problem': f'{dormant_users:,.0f} users at risk of going dormant (‚Çπ{dormant_users*100:,.0f} wasted investment)',
    'solution': '30-day SMS drip campaign targeting inactive users',
    'implementation': [
        'Day 1: "Complete your Aadhaar - 3 easy steps"',
        'Day 7: "Get benefits faster with updated Aadhaar"',
        'Day 14: "Last chance - Update deadline approaching"',
        'Day 30: "Visit nearest center - locations here"'
    ],
    'cost_inr_cr': (dormant_users * 0.50 * 4) / 10000000,  # 4 SMS per user
    'timeline_days': 30,
    'impact': f'{dormant_users*0.4:,.0f} users recovered (40% conversion)',
    'roi': f'{(dormant_users*0.4*100) / ((dormant_users*0.50*4)/10000000):.1f}X',
    'kpi': 'Update rate > 5 per enrollment',
    'priority': 'URGENT',
    'stakeholders': ['UIDAI', 'State Governments', 'Telecom Partners']
}
recommendations.append(reactivation_recommendation)

print(f"\nüìã {reactivation_recommendation['id']}: {reactivation_recommendation['title']}")
print(f"   Problem: {reactivation_recommendation['problem']}")
print(f"   Impact: {reactivation_recommendation['impact']}")
print(f"   ROI: {reactivation_recommendation['roi']}")

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# RECOMMENDATION 2: Short-term Initiatives (30-90 days)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

print("\n\nüéØ TIER 2: SHORT-TERM INITIATIVES (30-90 Days)")
print("="*70)

# 2A: Fraud Detection System
fraud_recommendation = {
    'id': 'REC-003',
    'title': 'Real-time Fraud Detection System',
    'problem': '45% single-day pincodes, impossible age distributions, weekend anomalies',
    'solution': 'ML-based anomaly detection with real-time alerts',
    'implementation': [
        '1. Deploy ML model to flag suspicious patterns',
        '2. Auto-block transactions with risk score > 80',
        '3. Manual review queue for score 50-80',
        '4. State-wise fraud dashboards'
    ],
    'cost_inr_cr': 8.0,
    'timeline_days': 60,
    'impact': 'Prevent ‚Çπ100+ Cr annual fraud',
    'roi': '12.5X (‚Çπ100 Cr saved / ‚Çπ8 Cr investment)',
    'kpi': 'Fraud rate < 2%',
    'priority': 'HIGH',
    'stakeholders': ['Security Team', 'Data Science Team', 'Audit Department']
}
recommendations.append(fraud_recommendation)

print(f"\nüìã {fraud_recommendation['id']}: {fraud_recommendation['title']}")
print(f"   Problem: {fraud_recommendation['problem']}")
print(f"   Impact: {fraud_recommendation['impact']}")
print(f"   ROI: {fraud_recommendation['roi']}")

# 2B: Lighthouse Expansion
lighthouse_recommendation = {
    'id': 'REC-004',
    'title': 'Lighthouse Pincode Expansion Program',
    'problem': 'Only 347 lighthouses driving most activity - geographic inequality',
    'solution': 'Identify and activate 200 new lighthouse pincodes in underserved areas',
    'implementation': [
        '1. Identify top 200 pincodes in bottom 15 states',
        '2. Deploy mobile enrollment units',
        '3. Partner with local influencers/leaders',
        '4. Host enrollment drives with incentives'
    ],
    'cost_inr_cr': 15.0,
    'timeline_days': 90,
    'impact': '5M new users via geographic ripple effect',
    'roi': '3.3X (‚Çπ50 Cr value / ‚Çπ15 Cr investment)',
    'kpi': '200 new lighthouses, 25K enrollments each',
    'priority': 'HIGH',
    'stakeholders': ['Field Operations', 'State Coordinators', 'NGO Partners']
}
recommendations.append(lighthouse_recommendation)

print(f"\nüìã {lighthouse_recommendation['id']}: {lighthouse_recommendation['title']}")
print(f"   Impact: {lighthouse_recommendation['impact']}")

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# RECOMMENDATION 3: Long-term Strategies (90-365 days)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

print("\n\nüèóÔ∏è TIER 3: LONG-TERM STRATEGIES (90-365 Days)")
print("="*70)

# 3A: Equity Program
equity_recommendation = {
    'id': 'REC-005',
    'title': 'Digital Inclusion for Remote States',
    'problem': 'J&K, HP, NE states have 5X lower coverage than national average',
    'solution': 'Mobile enrollment vans + local language support + offline capability',
    'implementation': [
        '1. Deploy 500 mobile vans to remote areas',
        '2. Hire 2000 local enrollment officers',
        '3. Offline enrollment with periodic sync',
        '4. Vernacular language interfaces'
    ],
    'cost_inr_cr': 50.0,
    'timeline_days': 365,
    'impact': '2.5M underserved citizens enrolled',
    'roi': 'Social impact > financial (inclusive growth)',
    'kpi': 'All states > 100 enrollments/pincode',
    'priority': 'MEDIUM',
    'stakeholders': ['Ministry of Rural Development', 'State Governments', 'UIDAI']
}
recommendations.append(equity_recommendation)

print(f"\nüìã {equity_recommendation['id']}: {equity_recommendation['title']}")
print(f"   Problem: {equity_recommendation['problem']}")
print(f"   Impact: {equity_recommendation['impact']}")

# 3B: Nagaland Model Replication
nagaland_recommendation = {
    'id': 'REC-006',
    'title': 'Nagaland Success Model Replication',
    'problem': '15 states with <50K enrollments (vs Nagaland 350K% growth)',
    'solution': 'Study and replicate Nagaland April 2025 campaign',
    'implementation': [
        '1. Conduct Nagaland case study (what worked?)',
        '2. Adapt for 15 low-performing states',
        '3. Community mobilization + local partnerships',
        '4. Incentive programs for early adopters'
    ],
    'cost_inr_cr': 25.0,
    'timeline_days': 270,
    'impact': '10X growth in target states = 5M new enrollments',
    'roi': '2X (‚Çπ50 Cr value / ‚Çπ25 Cr investment)',
    'kpi': '10X enrollment growth in 15 states',
    'priority': 'MEDIUM',
    'stakeholders': ['State Chief Ministers', 'District Collectors', 'CSR Partners']
}
recommendations.append(nagaland_recommendation)

print(f"\nüìã {nagaland_recommendation['id']}: {nagaland_recommendation['title']}")
print(f"   Impact: {nagaland_recommendation['impact']}")

# ============================================================================
# CREATE RECOMMENDATION SUMMARY TABLE
# ============================================================================

rec_df = pd.DataFrame(recommendations)

print("\n\n" + "="*80)
print("üìä COMPLETE RECOMMENDATION PORTFOLIO")
print("="*80)

summary_table = rec_df[['id', 'title', 'cost_inr_cr', 'timeline_days', 'impact', 'priority']].copy()
summary_table.columns = ['ID', 'Recommendation', 'Cost (‚Çπ Cr)', 'Timeline (Days)', 'Impact', 'Priority']

print("\n" + summary_table.to_string(index=False))

total_investment = rec_df['cost_inr_cr'].sum()
print(f"\nüí∞ TOTAL PORTFOLIO INVESTMENT: ‚Çπ{total_investment:.2f} Crore")
print(f"üìà TOTAL EXPECTED IMPACT: 12M+ citizens reached, ‚Çπ100+ Cr fraud prevented")
print(f"üéØ OVERALL ROI: 8-10X")

# ============================================================================
# CREATE IMPLEMENTATION ROADMAP
# ============================================================================

print("\n\n" + "="*80)
print("üóìÔ∏è IMPLEMENTATION ROADMAP (Q1-Q4 2026)")
print("="*80)

roadmap = {
    'Q1 2026 (Jan-Mar)': ['REC-001: Tuesday Load Fix', 'REC-002: Dormant User Campaign'],
    'Q2 2026 (Apr-Jun)': ['REC-003: Fraud Detection System', 'REC-004: Lighthouse Expansion Pilot'],
    'Q3 2026 (Jul-Sep)': ['REC-004: Lighthouse Full Rollout', 'REC-006: Nagaland Model Start'],
    'Q4 2026 (Oct-Dec)': ['REC-005: Equity Program Launch', 'REC-006: Nagaland Model Scale']
}

for quarter, activities in roadmap.items():
    print(f"\n{quarter}:")
    for activity in activities:
        print(f"  ‚úì {activity}")

# ============================================================================
# VISUALIZATION
# ============================================================================

fig, axes = plt.subplots(2, 2, figsize=(16, 12))

# 1. Cost breakdown
axes[0,0].barh(rec_df['title'], rec_df['cost_inr_cr'],
               color=['#E63946' if p == 'CRITICAL' else '#F18F01' if p == 'URGENT' else '#06A77D' for p in rec_df['priority']])
axes[0,0].set_xlabel('Investment (‚Çπ Crore)', fontsize=11)
axes[0,0].set_title('üí∞ Investment Required by Recommendation', fontsize=12, fontweight='bold')
axes[0,0].grid(True, alpha=0.3, axis='x')

# 2. Timeline
axes[0,1].barh(rec_df['title'], rec_df['timeline_days'],
               color='#2E86AB', alpha=0.8)
axes[0,1].set_xlabel('Timeline (Days)', fontsize=11)
axes[0,1].set_title('‚è±Ô∏è Implementation Timeline', fontsize=12, fontweight='bold')
axes[0,1].grid(True, alpha=0.3, axis='x')

# 3. Priority distribution
priority_counts = rec_df['priority'].value_counts()
colors_priority = {'CRITICAL': '#E63946', 'URGENT': '#F18F01', 'HIGH': '#06A77D', 'MEDIUM': '#2E86AB'}
axes[1,0].pie(priority_counts, labels=priority_counts.index, autopct='%1.0f%%',
              colors=[colors_priority[p] for p in priority_counts.index], startangle=90)
axes[1,0].set_title('üéØ Priority Distribution', fontsize=12, fontweight='bold')

# 4. Quarterly roadmap
quarters = list(roadmap.keys())
activities_per_quarter = [len(roadmap[q]) for q in quarters]
axes[1,1].bar(range(len(quarters)), activities_per_quarter,
              color='#06A77D', edgecolor='black', linewidth=2, alpha=0.8)
axes[1,1].set_xticks(range(len(quarters)))
axes[1,1].set_xticklabels(['Q1', 'Q2', 'Q3', 'Q4'])
axes[1,1].set_ylabel('Number of Initiatives', fontsize=11)
axes[1,1].set_title('üóìÔ∏è Implementation Roadmap 2026', fontsize=12, fontweight='bold')
axes[1,1].grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.savefig('Recommendations_Portfolio.png', dpi=300, bbox_inches='tight')
plt.show()

# Save to CSV
rec_df.to_csv('Recommendations_Portfolio.csv', index=False)

print("\n‚úÖ Recommendations saved!")
print("\n" + "="*80)